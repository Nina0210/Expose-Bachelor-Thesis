
@article{wu_efficient_2024,
	title = {Efficient and {Accurate} {PageRank} {Approximation} on {Large} {Graphs}},
	volume = {2},
	url = {https://dl.acm.org/doi/10.1145/3677132},
	doi = {10.1145/3677132},
	abstract = {PageRank is a commonly used measurement in a wide range of applications, including search engines, recommendation systems, and social networks. However, this measurement suffers from huge computational overhead, which cannot be scaled to large graphs. Although many approximate algorithms have been proposed for computing PageRank values, these algorithms are either (i) not efficient or (ii) not accurate. Worse still, some of them cannot provide estimated PageRank values for all the vertices. In this paper, we first propose the CUR-Trans algorithm, which can reduce the time complexity for computing PageRank values and has lower error bound than existing matrix approximation-based PageRank algorithms. Then, we develop the T 2-Approx algorithm to further reduce the time complexity for computing this measurement. Experiment results on three large-scale graphs show that both the CUR-Trans algorithm and the T 2-Approx algorithm achieve the lowest response time for computing PageRank values with the best accuracy (for the CUR-Trans algorithm) or the competitive accuracy (for the T 2-Approx algorithm). Besides, the two proposed algorithms are able to provide estimated PageRank values for all the vertices.},
	number = {4},
	urldate = {2025-03-24},
	journal = {Proc. ACM Manag. Data},
	author = {Wu, Siyue and Wu, Dingming and Quan, Junyi and Chan, Tsz Nam and Lu, Kezhong},
	month = sep,
	year = {2024},
	pages = {196:1--196:26},
}

@article{bahmani_fast_2010,
	title = {Fast incremental and personalized {PageRank}},
	volume = {4},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1929861.1929864},
	doi = {10.14778/1929861.1929864},
	abstract = {In this paper, we analyze the efficiency of Monte Carlo methods for incremental computation of PageRank, personalized PageRank, and similar random walk based methods (with focus on SALSA), on large-scale dynamically evolving social networks. We assume that the graph of friendships is stored in distributed shared memory, as is the case for large social networks such as Twitter.For global PageRank, we assume that the social network has n nodes, and m adversarially chosen edges arrive in a random order. We show that with a reset probability of ε, the expected total work needed to maintain an accurate estimate (using the Monte Carlo method) of the PageRank of every node at all times is [EQUATION]. This is significantly better than all known bounds for incremental PageRank. For instance, if we naively recompute the PageRanks as each edge arrives, the simple power iteration method needs [EQUATION] total time and the Monte Carlo method needs O(mn/ε) total time; both are prohibitively expensive. We also show that we can handle deletions equally efficiently.We then study the computation of the top k personalized PageRanks starting from a seed node, assuming that personalized PageRanks follow a power-law with exponent α \&lt; 1. We show that if we store R \&gt; q ln n random walks starting from every node for large enough constant q (using the approach outlined for global PageRank), then the expected number of calls made to the distributed social network database is O(k/(R(1-α)/α)). We also present experimental results from the social networking site, Twitter, verifying our assumptions and analyses. The overall result is that this algorithm is fast enough for real-time queries over a dynamic social network.},
	number = {3},
	urldate = {2025-03-24},
	journal = {Proc. VLDB Endow.},
	author = {Bahmani, Bahman and Chowdhury, Abdur and Goel, Ashish},
	year = {2010},
	pages = {173--184},
}

@article{hou_personalized_2023,
	title = {Personalized {PageRank} on {Evolving} {Graphs} with an {Incremental} {Index}-{Update} {Scheme}},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3588705},
	doi = {10.1145/3588705},
	abstract = {{\textbackslash}em Personalized PageRank (PPR) stands as a fundamental proximity measure in graph mining. Given an input graph G with the probability of decay α, a source node s and a target node t, the PPR score π(s,t) of target t with respect to source s is the probability that an α-decay random walk starting from s stops at t. A {\textbackslash}em single-source PPR (SSPPR) query takes an input graph G with decay probability α and a source s, and then returns the PPR π(s,v) for each node v ∈ V. Since computing an exact SSPPR query answer is prohibitive, most existing solutions turn to approximate queries with guarantees. The state-of-the-art solutions for approximate SSPPR queries are index-based and mainly focus on static graphs, while real-world graphs are usually dynamically changing. However, existing index-update schemes can not achieve a sub-linear update time. Motivated by this, we present an efficient indexing scheme for single-source PPR queries on evolving graphs. Our proposed solution is based on a classic framework that combines the forward-push technique with a random walk index for approximate PPR queries. Thus, our indexing scheme is similar to existing solutions in the sense that we store pre-sampled random walks for efficient query processing. One of our main contributions is an incremental updating scheme to maintain indexed random walks in expected O(1) time after each graph update. To achieve O(1) update cost, we need to maintain auxiliary data structures for both vertices and edges. To reduce the space consumption, we further revisit the sampling methods and propose a new sampling scheme to remove the auxiliary data structure for vertices while still supporting O(1) index update cost on evolving graphs. Extensive experiments show that our update scheme achieves orders of magnitude speed-up on update performance over existing index-based dynamic schemes without sacrificing the query efficiency.},
	number = {1},
	urldate = {2025-03-24},
	journal = {Proc. ACM Manag. Data},
	author = {Hou, Guanhao and Guo, Qintian and Zhang, Fangyuan and Wang, Sibo and Wei, Zhewei},
	year = {2023},
	pages = {25:1--25:26},
}

@article{wu_efficient_2024-1,
	title = {Efficient and {Accurate} {PageRank} {Approximation} on {Large} {Graphs}},
	volume = {2},
	url = {https://dl.acm.org/doi/10.1145/3677132},
	doi = {10.1145/3677132},
	abstract = {PageRank is a commonly used measurement in a wide range of applications, including search engines, recommendation systems, and social networks. However, this measurement suffers from huge computational overhead, which cannot be scaled to large graphs. Although many approximate algorithms have been proposed for computing PageRank values, these algorithms are either (i) not efficient or (ii) not accurate. Worse still, some of them cannot provide estimated PageRank values for all the vertices. In this paper, we first propose the CUR-Trans algorithm, which can reduce the time complexity for computing PageRank values and has lower error bound than existing matrix approximation-based PageRank algorithms. Then, we develop the T 2-Approx algorithm to further reduce the time complexity for computing this measurement. Experiment results on three large-scale graphs show that both the CUR-Trans algorithm and the T 2-Approx algorithm achieve the lowest response time for computing PageRank values with the best accuracy (for the CUR-Trans algorithm) or the competitive accuracy (for the T 2-Approx algorithm). Besides, the two proposed algorithms are able to provide estimated PageRank values for all the vertices.},
	number = {4},
	urldate = {2025-03-21},
	journal = {Proc. ACM Manag. Data},
	author = {Wu, Siyue and Wu, Dingming and Quan, Junyi and Chan, Tsz Nam and Lu, Kezhong},
	month = sep,
	year = {2024},
	pages = {196:1--196:26},
}

@article{zhuo_distributed_2021,
	title = {Distributed {Graph} {Processing} {System} and {Processing}-in-memory {Architecture} with {Precise} {Loop}-carried {Dependency} {Guarantee}},
	volume = {37},
	issn = {0734-2071},
	url = {https://dl.acm.org/doi/10.1145/3453681},
	doi = {10.1145/3453681},
	abstract = {To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—GraphS and GraphSR—for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.},
	number = {1-4},
	urldate = {2025-03-17},
	journal = {ACM Trans. Comput. Syst.},
	author = {Zhuo, Youwei and Chen, Jingji and Rao, Gengyu and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
	month = jul,
	year = {2021},
	pages = {5:1--5:37},
}

@misc{noauthor_sync_nodate,
	title = {{SYNC} or {ASYNC}: time to fuse for distributed graph-parallel computation {\textbar} {Proceedings} of the 20th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	url = {https://dl.acm.org/doi/10.1145/2688500.2688508},
	urldate = {2025-03-17},
}

@misc{noauthor_chaos_nodate,
	title = {Chaos {\textbar} {Proceedings} of the 25th {Symposium} on {Operating} {Systems} {Principles}},
	url = {https://dl.acm.org/doi/10.1145/2815400.2815408},
	urldate = {2025-03-17},
}

@article{gong_automating_2021,
	title = {Automating incremental graph processing with flexible memoization},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3461535.3461550},
	doi = {10.14778/3461535.3461550},
	abstract = {The ever-growing amount of dynamic graph data demands efficient techniques of incremental graph processing. However, incremental graph algorithms are challenging to develop. Existing approaches usually require users to manually design nontrivial incremental operators, or choose different memoization strategies for certain specific types of computation, limiting the usability and generality.In light of these challenges, we propose Ingress, an automated system for \&lt;u\&gt;in\&lt;/u\&gt;cremental \&lt;u\&gt;g\&lt;/u\&gt;raph proc\&lt;u\&gt;ess\&lt;/u\&gt;ing. Ingress is able to incrementalize batch vertex-centric algorithms into their incremental counterparts as a whole, without the need of redesigned logic or data structures from users. Underlying Ingress is an automated incrementalization framework equipped with four different memoization policies, to support all kinds of vertex-centric computations with optimized memory utilization. We identify sufficient conditions for the applicability of these policies. Ingress chooses the best-fit policy for a given algorithm automatically by verifying these conditions. In addition to the ease-of-use and generalization, Ingress outperforms state-of-the-art incremental graph systems by 15.93X on average (up to 147.14X) in efficiency.},
	number = {9},
	urldate = {2025-03-17},
	journal = {Proc. VLDB Endow.},
	author = {Gong, Shufeng and Tian, Chao and Yin, Qiang and Yu, Wenyuan and Zhang, Yanfeng and Geng, Liang and Yu, Song and Yu, Ge and Zhou, Jingren},
	year = {2021},
	pages = {1613--1625},
}

@inproceedings{zhu_gemini_2016,
	address = {USA},
	series = {{OSDI}'16},
	title = {Gemini: a computation-centric distributed graph processing system},
	isbn = {978-1-931971-33-1},
	shorttitle = {Gemini},
	abstract = {Traditionally distributed graph processing systems have largely focused on scalability through the optimizations of inter-node communication and load balance. However, they often deliver unsatisfactory overall processing efficiency compared with shared-memory graph computing frameworks. We analyze the behavior of several graph-parallel systems and find that the added overhead for achieving scalability becomes a major limiting factor for efficiency, especially with modern multi-core processors and high-speed interconnection networks.Based on our observations, we present Gemini, a distributed graph processing system that applies multiple optimizations targeting computation performance to build scalability on top of efficiency. Gemini adopts (1) a sparse-dense signal-slot abstraction to extend the hybrid push-pull computation model from shared-memory to distributed scenarios, (2) a chunk-based partitioning scheme enabling low-overhead scaling out designs and locality-preserving vertex accesses, (3) a dual representation scheme to compress accesses to vertex indices, (4) NUMA-aware sub-partitioning for efficient intra-node memory accesses, plus (5) locality-aware chunking and fine-grained work-stealing for improving both internode and intra-node load balance, respectively. Our evaluation on an 8-node high-performance cluster (using five widely used graph applications and five real-world graphs) shows that Gemini significantly outperforms all well-known existing distributed graph processing systems, delivering up to 39.8× (from 8.91×) improvement over the fastest among them.},
	urldate = {2025-03-17},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Zhu, Xiaowei and Chen, Wenguang and Zheng, Weimin and Ma, Xiaosong},
	month = nov,
	year = {2016},
	pages = {301--316},
}

@inproceedings{gonzalez_powergraph_2012,
	address = {USA},
	series = {{OSDI}'12},
	title = {{PowerGraph}: distributed graph-parallel computation on natural graphs},
	isbn = {978-1-931971-96-6},
	shorttitle = {{PowerGraph}},
	abstract = {Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability.In this paper, we characterize the challenges of computation on natural graphs in the context of existing graph-parallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.},
	urldate = {2025-03-17},
	booktitle = {Proceedings of the 10th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Gonzalez, Joseph E. and Low, Yucheng and Gu, Haijie and Bickson, Danny and Guestrin, Carlos},
	year = {2012},
	pages = {17--30},
}

@misc{noauthor_software_nodate,
	title = {Software {Systems} {Implementation} and {Domain}-{Specific} {Architectures} towards {Graph} {Analytics}},
	url = {https://spj.science.org/doi/10.34133/2022/9806758},
	language = {en},
	urldate = {2025-03-17},
	doi = {10.34133/2022/9806758},
}

@article{gong_automating_2021-1,
	title = {Automating incremental graph processing with flexible memoization},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3461535.3461550},
	doi = {10.14778/3461535.3461550},
	abstract = {The ever-growing amount of dynamic graph data demands efficient techniques of incremental graph processing. However, incremental graph algorithms are challenging to develop. Existing approaches usually require users to manually design nontrivial incremental operators, or choose different memoization strategies for certain specific types of computation, limiting the usability and generality.In light of these challenges, we propose Ingress, an automated system for \&lt;u\&gt;in\&lt;/u\&gt;cremental \&lt;u\&gt;g\&lt;/u\&gt;raph proc\&lt;u\&gt;ess\&lt;/u\&gt;ing. Ingress is able to incrementalize batch vertex-centric algorithms into their incremental counterparts as a whole, without the need of redesigned logic or data structures from users. Underlying Ingress is an automated incrementalization framework equipped with four different memoization policies, to support all kinds of vertex-centric computations with optimized memory utilization. We identify sufficient conditions for the applicability of these policies. Ingress chooses the best-fit policy for a given algorithm automatically by verifying these conditions. In addition to the ease-of-use and generalization, Ingress outperforms state-of-the-art incremental graph systems by 15.93X on average (up to 147.14X) in efficiency.},
	number = {9},
	urldate = {2025-03-15},
	journal = {Proc. VLDB Endow.},
	author = {Gong, Shufeng and Tian, Chao and Yin, Qiang and Yu, Wenyuan and Zhang, Yanfeng and Geng, Liang and Yu, Song and Yu, Ge and Zhou, Jingren},
	year = {2021},
	pages = {1613--1625},
}

@article{qian_graph_2021,
	title = {Graph processing and machine learning architectures with emerging memory technologies: a survey},
	volume = {64},
	issn = {1869-1919},
	shorttitle = {Graph processing and machine learning architectures with emerging memory technologies},
	url = {https://doi.org/10.1007/s11432-020-3219-6},
	doi = {10.1007/s11432-020-3219-6},
	abstract = {This paper surveys domain-specific architectures (DSAs) built from two emerging memory technologies. Hybrid memory cube (HMC) and high bandwidth memory (HBM) can reduce data movement between memory and computation by placing computing logic inside memory dies. On the other hand, the emerging non-volatile memory, metal-oxide resistive random access memory (ReRAM) has been considered as a promising candidate for future memory architecture due to its high density, fast read access and low leakage power. The key feature is ReRAM’s capability to perform the inherently parallel in-situ matrix-vector multiplication in the analog domain. We focus on the DSAs for two important applications—graph processing and machine learning acceleration. Based on the understanding of the recent architectures and our research experience, we also discuss several potential research directions.},
	language = {en},
	number = {6},
	urldate = {2025-03-14},
	journal = {Science China Information Sciences},
	author = {Qian, Xuehai},
	month = may,
	year = {2021},
	keywords = {HMC/HBM, ReRAM, graph processing, machine learning acceleration},
	pages = {160401},
}

@article{hu_edge_2023,
	title = {An edge re-ordering based acceleration architecture for improving data locality in graph analytics applications},
	volume = {102},
	issn = {0141-9331},
	url = {https://www.sciencedirect.com/science/article/pii/S0141933123001394},
	doi = {10.1016/j.micpro.2023.104895},
	abstract = {Data structure is the key in Edge Computing where various types of data are continuously generated by ubiquitous devices. Within all common data structures, graphs are used to express relationships and dependencies among human identities, objects, and locations. They are also expected to become one of the most important data infrastructures in the near future. Furthermore, as graph processing often requires random accesses to vast memory spaces, conventional memory hierarchies with caches cannot work efficiently. To alleviate such memory access bottlenecks in graph processing, we present a solution through vertex accesses scheduling and edge array re-ordering, in parallel with the execution of graph processing applications to improve both temporal and spatial locality of memory accesses, especially for edge-centric graph analytics which are popular means in handling dynamic graphs. Our proposed architecture is evaluated and tested through both trace-based cache simulations and cycle-accurate FPGA-based prototyping. Evaluation results show that our proposal has a potential of significantly reducing the Last Level Cache (LLC) misses by 62.60\% in general among PageRank and BFS algorithms. Meanwhile, evaluations with the FPGA prototype successfully reduce the quantity of Miss-Per-Kilo-Instructions (MPKI) for LLC by 56.27\% on average.},
	urldate = {2025-03-14},
	journal = {Microprocessors and Microsystems},
	author = {Hu, Siyi and Kondo, Masaaki and He, Yuan and Sakamoto, Ryuichi and Zhang, Hao and Zhou, Jun and Nakamura, Hiroshi},
	month = oct,
	year = {2023},
	keywords = {Cache, Data locality, Domain-specific acceleration, Graph processing},
	pages = {104895},
}

@inproceedings{su_graflex_2024,
	address = {New York, NY, USA},
	series = {{FPGA} '24},
	title = {{GraFlex}: {Flexible} {Graph} {Processing} on {FPGAs} through {Customized} {Scalable} {Interconnection} {Network}},
	isbn = {9798400704185},
	shorttitle = {{GraFlex}},
	url = {https://doi.org/10.1145/3626202.3637573},
	doi = {10.1145/3626202.3637573},
	abstract = {Graph processing system design has been widely considered to be a challenging topic due to the mismatch between the computational throughput requirement and the memory bandwidth. Recent works try to deliver better graph processing systems by taking advantage of application-specific architectures and emerging high-bandwidth memory on FPGAs. However, there is still ample room for improvements regarding flexibility, scalability, and usability. This paper presents GraFlex, a flexible scatter-gather graph processing framework on FPGAs with scalable interconnection networks. It adopts the Bulk-Synchronous Parallel (BSP) paradigm for global control and synchronization, enabling rapid deployment of performant graph processing systems through HLS-based design flows. GraFlex conducts software-hardware co-optimization to boost system performance. It configures the compact graph format, partition scheme, and memory channel allocation strategy to support scalable designs. Resource-efficient multi-stage butterfly interconnection network achieves on-device data communication and facilitates throughput matching. To handle fragmented memory requests, we propose coalesced memory access engines to improve bandwidth utilization. GraFlex is comprehensively evaluated with various graph applications and real-world datasets. Our results show up to 2.09{\textbackslash}texttimes average speedup in traversal throughput over the existing state-of-the-art work with a non-negligible reduction in power and resource consumption. A case study of the breadth-first search (BFS) application shows a 6.58{\textbackslash}texttimes speedup in average algorithm throughout with proper implementation choices enabled by the scatter-gather mechanism implemented. The BFS study also reports an almost linear throughput scaling versus the number of processing elements (PEs) and memory channels.},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the 2024 {ACM}/{SIGDA} {International} {Symposium} on {Field} {Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Su, Chunyou and Du, Linfeng and Liang, Tingyuan and Lin, Zhe and Wang, Maolin and Sinha, Sharad and Zhang, Wei},
	month = apr,
	year = {2024},
	pages = {143--153},
}

@article{yu_multipim_2021,
	title = {{MultiPIM}: {A} {Detailed} and {Configurable} {Multi}-{Stack} {Processing}-{In}-{Memory} {Simulator}},
	volume = {20},
	issn = {1556-6064},
	shorttitle = {{MultiPIM}},
	url = {https://ieeexplore.ieee.org/document/9362242},
	doi = {10.1109/LCA.2021.3061905},
	abstract = {Processing-in-Memory (PIM) has being actively studied as a promising solution to overcome the memory wall problem. Therefore, there is an urgent need for a PIM simulation infrastructure to help researchers quickly understand existing problems and verify new mechanisms. However, existing PIM simulators do not consider architectural details and the programming interface that are necessary for a practical PIM system. In this letter, we present MultiPIM, a PIM simulator that models microarchitectural details that stem from supporting multiple memory stacks and massively-parallel PIM cores. On top of the detailed simulation infrastructure, MultiPIM provides an easy-to-use interface for configuring PIM hardware and adapting existing workloads for PIM offloading.},
	number = {1},
	urldate = {2025-03-14},
	journal = {IEEE Computer Architecture Letters},
	author = {Yu, Chao and Liu, Sihang and Khan, Samira},
	month = jan,
	year = {2021},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Control systems, Memory, Processing-in-memory, Random access memory, Switches, Task analysis, Timing, Topology, memory network, simulator},
	pages = {54--57},
}

@inproceedings{ham_graphicionado_2016,
	title = {Graphicionado: {A} high-performance and energy-efficient accelerator for graph analytics},
	shorttitle = {Graphicionado},
	url = {https://ieeexplore.ieee.org/document/7783759},
	doi = {10.1109/MICRO.2016.7783759},
	abstract = {Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76–6.54x speedup while consuming 50–100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.},
	urldate = {2025-03-13},
	booktitle = {2016 49th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Ham, Tae Jun and Wu, Lisa and Sundaram, Narayanan and Satish, Nadathur and Martonosi, Margaret},
	month = oct,
	year = {2016},
	keywords = {Machine learning algorithms, Memory management, Pipelines, Programming, Software, Software algorithms, System-on-chip},
	pages = {1--13},
}

@inproceedings{zhang_graphp_2018,
	title = {{GraphP}: {Reducing} {Communication} for {PIM}-{Based} {Graph} {Processing} with {Efficient} {Data} {Partition}},
	shorttitle = {{GraphP}},
	url = {https://ieeexplore.ieee.org/document/8327036},
	doi = {10.1109/HPCA.2018.00053},
	abstract = {Processing-In-Memory (PIM) is an effective technique that reduces data movements by integrating processing units within memory. The recent advance of “big data” and 3D stacking technology make PIM a practical and viable solution for the modern data processing workloads. It is exemplified by the recent research interests on PIM-based acceleration. Among them, TESSERACT is a PIM-enabled parallel graph processing architecture based on Micron’s Hybrid Memory Cube (HMC), one of the most prominent 3D-stacked memory technologies. It implements a Pregel-like vertex-centric programming model, so that users could develop programs in the familiar interface while taking advantage of PIM. Despite the orders of magnitude speedup compared to DRAM-based systems, TESSERACT generates excessive crosscube communications through SerDes links, whose bandwidth is much less than the aggregated local bandwidth of HMCs. Our investigation indicates that this is because of the restricted data organization required by the vertex programming model. In this paper, we argue that a PIM-based graph processing system should take data organization as a first-order design consideration. Following this principle, we propose GraphP, a novel HMC-based software/hardware co-designed graph processing system that drastically reduces communication and energy consumption compared to TESSERACT. GraphP features three key techniques. 1) “Source-cut” partitioning, which fundamentally changes the cross-cube communication from one remote put per cross-cube edge to one update per replica. 2) “Two-phase Vertex Program”, a programming model designed for the “source-cut” partitioning with two operations: GenUpdate and ApplyUpdate. 3) Hierarchical communication and overlapping, which further improves performance with unique opportunities offered by the proposed partitioning and programming model. We evaluate GraphP using a cycle accurate simulator with 5 real-world graphs and 4 algorithms. The results show that it provides on average 1.7 speedup and 89\% energy saving compared to TESSERACT.},
	urldate = {2025-03-10},
	booktitle = {2018 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Zhang, Mingxing and Zhuo, Youwei and Wang, Chao and Gao, Mingyu and Wu, Yongwei and Chen, Kang and Kozyrakis, Christos and Qian, Xuehai},
	month = feb,
	year = {2018},
	note = {ISSN: 2378-203X},
	keywords = {Bandwidth, Graph processing, Hybrid Memory Cube, Memory management, Organizations, Partitioning algorithms, Processing In Memory, Programming, Three-dimensional displays},
	pages = {544--557},
}

@inproceedings{li_optimal_2022,
	address = {Taipei, Taiwan},
	series = {{ASPDAC} '22},
	title = {Optimal {Data} {Allocation} for {Graph} {Processing} in {Processing}-in-{Memory} {Systems}},
	isbn = {978-1-66542-135-5},
	url = {https://dl.acm.org/doi/10.1109/ASP-DAC52403.2022.9712587},
	doi = {10.1109/ASP-DAC52403.2022.9712587},
	abstract = {Graph processing involves lots of irregular memory accesses and increases demands on high memory bandwidth, making it difficult to execute efficiently on compute-centric architectures. Dedicated graph processing accelerators based on the processing-in-memory (PIM) technique have recently been proposed. Despite they achieved higher performance and energy efficiency than conventional architectures, the data allocation problem for communication minimization in PIM systems (e.g., hybrid memory cubes (HMCs)) has still not been well solved. In this paper, we demonstrate that the conventional "graph data allocation = graph partitioning" assumption is not true, and the memory access patterns of graph algorithms should also be taken into account when partitioning graph data for communication minimization. For this purpose, we classify graph algorithms into two representative classes from a memory access pattern point of view and propose different graph data partitioning strategies for them. We then propose two algorithms to optimize the partition-to-HMC mapping to minimize the inter-HMC communication. Evaluations have proved the superiority of our data allocation framework and the data movement energy efficiency is improved by 4.2--5× on average than the state-of-the-art GraphP approach.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 27th {Asia} and {South} {Pacific} {Design} {Automation} {Conference}},
	publisher = {IEEE Press},
	author = {Li, Zerun and Chen, Xiaoming and Han, Yinhe},
	month = jan,
	year = {2022},
	pages = {238--243},
}

@inproceedings{malewicz_pregel_2010,
	address = {New York, NY, USA},
	series = {{SIGMOD} '10},
	title = {Pregel: a system for large-scale graph processing},
	isbn = {978-1-4503-0032-2},
	shorttitle = {Pregel},
	url = {https://dl.acm.org/doi/10.1145/1807167.1807184},
	doi = {10.1145/1807167.1807184},
	abstract = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2010 {ACM} {SIGMOD} {International} {Conference} on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J.C and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
	month = jun,
	year = {2010},
	pages = {135--146},
}

@inproceedings{will_ruya_2022,
	title = {Ruya: {Memory}-{Aware} {Iterative} {Optimization} of {Cluster} {Configurations} for {Big} {Data} {Processing}},
	shorttitle = {Ruya},
	url = {https://ieeexplore.ieee.org/document/10020295},
	doi = {10.1109/BigData55660.2022.10020295},
	abstract = {Selecting appropriate computational resources for data processing jobs on large clusters is difficult, even for expert users like data engineers. Inadequate choices can result in vastly increased costs, without significantly improving performance. One crucial aspect of selecting an efficient resource configuration is avoiding memory bottlenecks. By knowing the required memory of a job in advance, the search space for an optimal resource configuration can be greatly reduced.Therefore, we present Ruya, a method for memory-aware optimization of data processing cluster configurations based on iteratively exploring a narrowed-down search space. First, we perform job profiling runs with small samples of the dataset on just a single machine to model the job’s memory usage patterns. Second, we prioritize cluster configurations with a suitable amount of total memory and within this reduced search space, we iteratively search for the best cluster configuration with Bayesian optimization. This search process stops once it converges on a configuration that is believed to be optimal for the given job. In our evaluation on a dataset with 1031 Spark and Hadoop jobs, we see a reduction of search iterations to find an optimal configuration by around half, compared to the baseline.},
	urldate = {2025-03-10},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Will, Jonathan and Thamsen, Lauritz and Bader, Jonathan and Scheinert, Dominik and Kao, Odej},
	month = dec,
	year = {2022},
	keywords = {Big Data, Cluster Management, Costs, Current measurement, Data models, Data processing, Distributed Dataflows, Memory management, Profiling, Resource Allocation, Scalable Data Analytics, Sparks},
	pages = {161--169},
}

@inproceedings{al-sayeh_juggler_2022,
	address = {Philadelphia PA USA},
	title = {Juggler: {Autonomous} {Cost} {Optimization} and {Performance} {Prediction} of {Big} {Data} {Applications}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {Juggler},
	url = {https://dl.acm.org/doi/10.1145/3514221.3517892},
	doi = {10.1145/3514221.3517892},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Al-Sayeh, Hani and Memishi, Bunjamin and Jibril, Muhammad Attahir and Paradies, Marcus and Sattler, Kai-Uwe},
	month = jun,
	year = {2022},
	pages = {1840--1854},
}

@misc{al-sayeh_blink_2022,
	title = {Blink: {Lightweight} {Sample} {Runs} for {Cost} {Optimization} of {Big} {Data} {Applications}},
	shorttitle = {Blink},
	url = {http://arxiv.org/abs/2207.02290},
	doi = {10.48550/arXiv.2207.02290},
	abstract = {Distributed in-memory data processing engines accelerate iterative applications by caching substantial datasets in memory rather than recomputing them in each iteration. Selecting a suitable cluster size for caching these datasets plays an essential role in achieving optimal performance. In practice, this is a tedious and hard task for end users, who are typically not aware of cluster specifications, workload semantics and sizes of intermediate data. We present Blink, an autonomous sampling-based framework, which predicts sizes of cached datasets and selects optimal cluster size without relying on historical runs. We evaluate Blink on a variety of iterative, real-world, machine learning applications. With an average sample runs cost of 4.6\% compared to the cost of optimal runs, Blink selects the optimal cluster size in 15 out of 16 cases, saving up to 47.4\% of execution cost compared to average costs.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Al-Sayeh, Hani and Jibril, Muhammad Attahir and Memishi, Bunjamin and Sattler, Kai-Uwe},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02290 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{waleffe_armada_2025,
	title = {Armada: {Memory}-{Efficient} {Distributed} {Training} of {Large}-{Scale} {Graph} {Neural} {Networks}},
	shorttitle = {Armada},
	url = {http://arxiv.org/abs/2502.17846},
	doi = {10.48550/arXiv.2502.17846},
	abstract = {We study distributed training of Graph Neural Networks (GNNs) on billion-scale graphs that are partitioned across machines. Efficient training in this setting relies on min-edge-cut partitioning algorithms, which minimize cross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cut partitioning over large graphs remains a challenge: State-of-the-art (SoTA) offline methods (e.g., METIS) are effective, but they require orders of magnitude more memory and runtime than GNN training itself, while computationally efficient algorithms (e.g., streaming greedy approaches) suffer from increased edge cuts. Thus, in this work we introduce Armada, a new end-to-end system for distributed GNN training whose key contribution is GREM, a novel min-edge-cut partitioning algorithm that can efficiently scale to large graphs. GREM builds on streaming greedy approaches with one key addition: prior vertex assignments are continuously refined during streaming, rather than frozen after an initial greedy selection. Our theoretical analysis and experimental results show that this refinement is critical to minimizing edge cuts and enables GREM to reach partition quality comparable to METIS but with 8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leverages a new disaggregated architecture for distributed GNN training to further improve efficiency; we find that on common cloud machines, even with zero communication, GNN neighborhood sampling and feature loading bottleneck training. Disaggregation allows Armada to independently allocate resources for these operations and ensure that expensive GPUs remain saturated with computation. We evaluate Armada against SoTA systems for distributed GNN training and find that the disaggregated architecture leads to runtime improvements up to 4.5x and cost reductions up to 3.1x.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Waleffe, Roger and Sarda, Devesh and Mohoney, Jason and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Rekatsinas, Theodoros and Venkataraman, Shivaram},
	month = feb,
	year = {2025},
	note = {arXiv:2502.17846 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@inproceedings{zhang_distributed_2021,
	address = {Singapore},
	title = {Distributed {Graph} {Processing}: {Techniques} and {Systems}},
	isbn = {9789811604799},
	shorttitle = {Distributed {Graph} {Processing}},
	doi = {10.1007/978-981-16-0479-9_2},
	abstract = {During the past 10 years, there has been a surging interest in developing distributed graph processing systems. This tutorial provides a comprehensive review of existing distributed graph processing systems. We firstly review the programming models for distributed graph processing and then summarize the common optimization techniques for improving graph execution performance, including graph partitioning methods, communication mechanisms, parallel processing models, hardware-specific optimizations, and incremental graph processing. We also present an emerging hot topic, distributed Graph Neural Networks (GNN) frameworks, and review recent progress on this topic.},
	language = {en},
	booktitle = {Web and {Big} {Data}. {APWeb}-{WAIM} 2020 {International} {Workshops}},
	publisher = {Springer},
	author = {Zhang, Yanfeng and Wang, Qiange and Gong, Shufeng},
	editor = {Chen, Qun and Li, Jianxin},
	year = {2021},
	keywords = {Distributed systems, Graph processing, Parallel models},
	pages = {14--23},
}

@inproceedings{xin_graphx_2013,
	address = {New York, NY, USA},
	series = {{GRADES} '13},
	title = {{GraphX}: a resilient distributed graph system on {Spark}},
	isbn = {978-1-4503-2188-4},
	shorttitle = {{GraphX}},
	url = {https://doi.org/10.1145/2484425.2484427},
	doi = {10.1145/2484425.2484427},
	abstract = {From social networks to targeted advertising, big graphs capture the structure in data and are central to recent advances in machine learning and data mining. Unfortunately, directly applying existing data-parallel tools to graph computation tasks can be cumbersome and inefficient. The need for intuitive, scalable tools for graph computation has lead to the development of new graph-parallel systems (e.g., Pregel, PowerGraph) which are designed to efficiently execute graph algorithms. Unfortunately, these new graph-parallel systems do not address the challenges of graph construction and transformation which are often just as problematic as the subsequent computation. Furthermore, existing graph-parallel systems provide limited fault-tolerance and support for interactive data mining.We introduce GraphX, which combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework. We leverage new ideas in distributed graph representation to efficiently distribute graphs as tabular data-structures. Similarly, we leverage advances in data-flow systems to exploit in-memory computation and fault-tolerance. We provide powerful new operations to simplify graph construction and transformation. Using these primitives we implement the PowerGraph and Pregel abstractions in less than 20 lines of code. Finally, by exploiting the Scala foundation of Spark, we enable users to interactively load, transform, and compute on massive graphs.},
	urldate = {2025-03-10},
	booktitle = {First {International} {Workshop} on {Graph} {Data} {Management} {Experiences} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xin, Reynold S. and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion},
	month = jun,
	year = {2013},
	pages = {1--6},
}

@article{zhuo_distributed_2019,
	title = {Distributed {Graph} {Processing} {System} and {Processing}-in-memory {Architecture} with {Precise} {Loop}-carried {Dependency} {Guarantee}},
	volume = {37},
	issn = {0734-2071, 1557-7333},
	url = {https://dl.acm.org/doi/10.1145/3453681},
	doi = {10.1145/3453681},
	abstract = {To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—
              GraphS
              and
              GraphSR
              —for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.},
	language = {en},
	number = {1-4},
	urldate = {2025-03-10},
	journal = {ACM Transactions on Computer Systems},
	author = {Zhuo, Youwei and Chen, Jingji and Rao, Gengyu and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
	month = nov,
	year = {2019},
	pages = {1--37},
}

@misc{meng_survey_2024,
	title = {A {Survey} of {Distributed} {Graph} {Algorithms} on {Massive} {Graphs}},
	url = {http://arxiv.org/abs/2404.06037},
	doi = {10.48550/arXiv.2404.06037},
	abstract = {Distributed processing of large-scale graph data has many practical applications and has been widely studied. In recent years, a lot of distributed graph processing frameworks and algorithms have been proposed. While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments. Applying graph tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth. In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed graph algorithms. We first conduct a systematic analysis of the inherent challenges in distributed graph processing, followed by presenting an overview of existing general solutions. Subsequently, we survey the challenges highlighted in recent distributed graph processing papers and the strategies adopted to address them. Finally, we discuss the current research trends and identify potential future opportunities.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Meng, Lingkai and Shao, Yu and Yuan, Long and Lai, Longbin and Cheng, Peng and Li, Xue and Yu, Wenyuan and Zhang, Wenjie and Lin, Xuemin and Zhou, Jingren},
	month = oct,
	year = {2024},
	note = {arXiv:2404.06037 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{baluja_video_2008,
	address = {New York, NY, USA},
	series = {{WWW} '08},
	title = {Video suggestion and discovery for youtube: taking random walks through the view graph},
	isbn = {978-1-60558-085-2},
	shorttitle = {Video suggestion and discovery for youtube},
	url = {https://dl.acm.org/doi/10.1145/1367497.1367618},
	doi = {10.1145/1367497.1367618},
	abstract = {The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them. Unfortunately, given the difficulty of searching videos, the size of the video repository also makes the discovery of new content a daunting task. In this paper, we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users. The resulting algorithm, termed Adsorption, provides a simple method to efficiently propagate preference information through a variety of graphs. We extensively test the results of the recommendations on a three month snapshot of live data from YouTube.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 17th international conference on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Baluja, Shumeet and Seth, Rohan and Sivakumar, D. and Jing, Yushi and Yagnik, Jay and Kumar, Shankar and Ravichandran, Deepak and Aly, Mohamed},
	month = apr,
	year = {2008},
	pages = {895--904},
}

@article{zhang_cgraph_2019,
	title = {{CGraph}: {A} {Distributed} {Storage} and {Processing} {System} for {Concurrent} {Iterative} {Graph} {Analysis} {Jobs}},
	volume = {15},
	issn = {1553-3077},
	shorttitle = {{CGraph}},
	url = {https://doi.org/10.1145/3319406},
	doi = {10.1145/3319406},
	abstract = {Distributed graph processing platforms usually need to handle massive Concurrent iterative Graph Processing (CGP) jobs for different purposes. However, existing distributed systems face high ratio of data access cost to computation for the CGP jobs, which incurs low throughput. We observed that there are strong spatial and temporal correlations among the data accesses issued by different CGP jobs, because these concurrently running jobs usually need to repeatedly traverse the shared graph structure for the iterative processing of each vertex. Based on this observation, this article proposes a distributed storage and processing system CGraph for the CGP jobs to efficiently handle the underlying static/evolving graph for high throughput. It uses a data-centric load-trigger-pushing model, together with several optimizations, to enable the CGP jobs to efficiently share the graph structure data in the cache/memory and their accesses by fully exploiting such correlations, where the graph structure data is decoupled from the vertex state associated with each job. It can deliver much higher throughput for the CGP jobs by effectively reducing their average ratio of data access cost to computation. Experimental results show that CGraph improves the throughput of the CGP jobs by up to 3.47× in comparison with existing solutions on distributed platforms.},
	number = {2},
	urldate = {2025-03-10},
	journal = {ACM Trans. Storage},
	author = {Zhang, Yu and Zhao, Jin and Liao, Xiaofei and Jin, Hai and Gu, Lin and Liu, Haikun and He, Bingsheng and He, Ligang},
	month = apr,
	year = {2019},
	pages = {10:1--10:26},
}

@article{wang_fargraph_2023,
	title = {Fargraph+: {Excavating} the parallelism of graph processing workload on {RDMA}-based far memory system},
	volume = {177},
	issn = {0743-7315},
	shorttitle = {Fargraph+},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731523000345},
	doi = {10.1016/j.jpdc.2023.02.015},
	abstract = {Disaggregated architecture brings new opportunities to memory-consuming applications like graph processing. It allows one to outspread memory access pressure from local to far memory, providing an attractive alternative to disk-based processing. Although existing works on general-purpose far memory platforms show great potentials for application expansion, it is unclear how graph processing applications could benefit from disaggregated architecture, and how different optimization methods influence the overall performance. In this paper, we take the first step to analyze the impact of graph processing workload on disaggregated architecture by extending the GridGraph framework on top of the RDMA-based far memory system. We propose Fargraph+, a system with parallel graph data offloading and far memory coordination strategy for enhancing efficiency of graph processing workload on RDMA-based far memory architecture. Specifically, Fargraph+ reduces the overall data movement through a well-crafted, graph-aware data segment offloading mechanism. In addition, we use optimal data segment splitting and asynchronous data buffering to achieve graph iteration-friendly far memory access. We further configure efficient parallelism-oriented control to accelerate performance of multi-threading processing on graph iterations while improving memory efficiency of far memory access by utilizing RDMA queue features. We show that Fargraph+ achieves near-oracle performance for typical in-local-memory graph processing systems. Fargraph+ shows up to 11.2× speedup compared to Fastswap, the state-of-the-art, general-purpose far memory platform.},
	urldate = {2025-03-10},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Jing and Li, Chao and Liu, Yibo and Wang, Taolei and Mei, Junyi and Zhang, Lu and Wang, Pengyu and Guo, Minyi},
	month = jul,
	year = {2023},
	keywords = {Far memory, Graph processing, RDMA},
	pages = {144--159},
}

@inproceedings{will_selecting_2023,
	address = {New York, NY, USA},
	series = {{SSDBM} '23},
	title = {Selecting {Efficient} {Cluster} {Resources} for {Data} {Analytics}: {When} and {How} to {Allocate} for {In}-{Memory} {Processing}?},
	isbn = {9798400707469},
	shorttitle = {Selecting {Efficient} {Cluster} {Resources} for {Data} {Analytics}},
	url = {https://dl.acm.org/doi/10.1145/3603719.3603733},
	doi = {10.1145/3603719.3603733},
	abstract = {Distributed dataflow systems such as Apache Spark or Apache Flink enable parallel, in-memory data processing on large clusters of commodity hardware. Consequently, the appropriate amount of memory to allocate to the cluster is a crucial consideration. In this paper, we analyze the challenge of efficient resource allocation for distributed data processing, focusing on memory. We\&nbsp;emphasize that in-memory processing with in-memory data processing frameworks can undermine resource efficiency. Based on the findings of our trace data analysis, we compile requirements towards an automated solution for efficient cluster resource allocation.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Will, Jonathan and Thamsen, Lauritz and Scheinert, Dominik and Kao, Odej},
	month = aug,
	year = {2023},
	pages = {1--4},
}

@misc{university_of_alabama_libraries_how_2021,
	title = {How to {Zotero}: {Adding} {Items} to {Your} {Library}},
	shorttitle = {How to {Zotero}},
	url = {https://www.youtube.com/watch?v=-cI23OS7724},
	abstract = {Creative Commons-Lizenz mit Quellenangabe (Wiederverwendung erlaubt)},
	urldate = {2025-03-10},
	author = {{University of Alabama Libraries}},
	month = aug,
	year = {2021},
}

@article{noauthor_notitle_nodate,
}
