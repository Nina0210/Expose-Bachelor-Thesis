
@inproceedings{weng_twitterrank_2010,
	address = {New York, NY, USA},
	series = {{WSDM} '10},
	title = {{TwitterRank}: finding topic-sensitive influential twitterers},
	isbn = {978-1-60558-889-6},
	shorttitle = {{TwitterRank}},
	url = {https://dl.acm.org/doi/10.1145/1718487.1718520},
	doi = {10.1145/1718487.1718520},
	abstract = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4\% of the users in Twitter follow more than 80\% of their followers, and (2) 80.5\% of the users have 80\% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
	urldate = {2025-04-14},
	booktitle = {Proceedings of the third {ACM} international conference on {Web} search and data mining},
	publisher = {Association for Computing Machinery},
	author = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
	month = feb,
	year = {2010},
	pages = {261--270},
}

@inproceedings{gonzalez_graphx_2014,
	title = {{GraphX}: {Graph} {Processing} in a {Distributed} {Dataflow} {Framework}},
	abstract = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataﬂow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataﬂow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataﬂow system. GraphX presents a familiar composable graph abstraction that is sufﬁcient to express existing graph APIs, yet can be implemented using only a few basic dataﬂow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-speciﬁc optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataﬂow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataﬂow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.},
	language = {en},
	publisher = {USENIX Association},
	author = {Gonzalez, Joseph E and Xin, Reynold S and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J and Stoica, Ion},
	year = {2014},
	pages = {599--613},
}

@inproceedings{gonzalez_graphx_2014-1,
	address = {USA},
	series = {{OSDI}'14},
	title = {{GraphX}: graph processing in a distributed dataflow framework},
	isbn = {978-1-931971-16-4},
	shorttitle = {{GraphX}},
	abstract = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataflow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataflow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphX presents a familiar composable graph abstraction that is sufficient to express existing graph APIs, yet can be implemented using only a few basic dataflow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-specific optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataflow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataflow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.},
	urldate = {2025-04-11},
	booktitle = {Proceedings of the 11th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Gonzalez, Joseph E. and Xin, Reynold S. and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J. and Stoica, Ion},
	year = {2014},
	pages = {599--613},
}

@inproceedings{bar-yossef_local_2008,
	address = {Napa Valley California USA},
	title = {Local approximation of pagerank and reverse pagerank},
	isbn = {978-1-59593-991-3},
	url = {https://dl.acm.org/doi/10.1145/1458082.1458122},
	doi = {10.1145/1458082.1458122},
	language = {en},
	urldate = {2025-04-11},
	booktitle = {Proceedings of the 17th {ACM} conference on {Information} and knowledge management},
	publisher = {ACM},
	author = {Bar-Yossef, Ziv and Mashiach, Li-Tal},
	month = oct,
	year = {2008},
	pages = {279--288},
}

@article{jin_software_2022,
	title = {Software {Systems} {Implementation} and {Domain}-{Specific} {Architectures} towards {Graph} {Analytics}},
	volume = {2022},
	url = {https://spj.science.org/doi/10.34133/2022/9806758},
	doi = {10.34133/2022/9806758},
	abstract = {Graph analytics, which mainly includes graph processing, graph mining, and graph learning, has become increasingly important in several domains, including social network analysis, bioinformatics, and machine learning. However, graph analytics applications suffer from poor locality, limited bandwidth, and low parallelism owing to the irregular sparse structure, explosive growth, and dependencies of graph data. To address those challenges, several programming models, execution modes, and messaging strategies are proposed to improve the utilization of traditional hardware and performance. In recent years, novel computing and memory devices have emerged, e.g., HMCs, HBM, and ReRAM, providing massive bandwidth and parallelism resources, making it possible to address bottlenecks in graph applications. To facilitate understanding of the graph analytics domain, our study summarizes and categorizes current software systems implementation and domain-specific architectures. Finally, we discuss the future challenges of graph analytics.},
	urldate = {2025-04-11},
	journal = {Intelligent Computing},
	author = {Jin, Hai and Qi, Hao and Zhao, Jin and Jiang, Xinyu and Huang, Yu and Gui, Chuangyi and Wang, Qinggang and Shen, Xinyang and Zhang, Yi and Hu, Ao and Chen, Dan and Liu, Chaoqiang and Liu, Haifeng and He, Haiheng and Ye, Xiangyu and Wang, Runze and Yuan, Jingrui and Yao, Pengcheng and Zhang, Yu and Zheng, Long and Liao, Xiaofei},
	month = oct,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
}

@inproceedings{khan_energy_2016,
	address = {Heidelberg Germany},
	title = {Energy efficiency of large scale graph processing platforms},
	isbn = {978-1-4503-4462-3},
	url = {https://dl.acm.org/doi/10.1145/2968219.2968296},
	doi = {10.1145/2968219.2968296},
	language = {en},
	urldate = {2025-04-11},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}: {Adjunct}},
	publisher = {ACM},
	author = {Khan, Kashif Nizam and Hoque, Mohammad Ashraful and Niemi, Tapio and Ou, Zhonghong and Nurminen, Jukka K.},
	month = sep,
	year = {2016},
	pages = {1287--1294},
}

@article{xu_graphscope_2021,
	title = {{GraphScope}: a one-stop large graph processing system},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{GraphScope}},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476324},
	doi = {10.14778/3476311.3476324},
	abstract = {Due to diverse graph data and algorithms, programming and orchestration of complex computation pipelines have become the major challenges to making use of graph applications for Web-scale data analysis. GraphScope aims to provide a one-stop and efficient solution for a wide range of graph computations at scale. It extends previous systems by offering a unified and high-level programming interface and allowing the seamless integration of specialized graph engines in a general data-parallel computing environment.As we will show in this demo, GraphScope enables developers to write sequential graph programs in Python and provides automatic parallel execution on a cluster. This further allows GraphScope to seamlessly integrate with existing data processing systems in PyData ecosystem. To validate GraphScope's efficiency, we will compare a complex, multi-staged processing pipeline for a real-life fraud detection task with a manually assembled implementation comprising multiple systems. GraphScope achieves a 2.86× speedup on a trillion-scale graph in real production at Alibaba.},
	number = {12},
	urldate = {2025-04-11},
	journal = {Proc. VLDB Endow.},
	author = {Xu, Jingbo and Bai, Zhanning and Fan, Wenfei and Lai, Longbin and Li, Xue and Li, Zhao and Qian, Zhengping and Wang, Lei and Wang, Yanyan and Yu, Wenyuan and Zhou, Jingren},
	month = jul,
	year = {2021},
	pages = {2703--2706},
}

@article{low_distributed_2012,
	title = {Distributed {GraphLab}: a framework for machine learning and data mining in the cloud},
	volume = {5},
	issn = {2150-8097},
	shorttitle = {Distributed {GraphLab}},
	url = {https://dl.acm.org/doi/10.14778/2212351.2212354},
	doi = {10.14778/2212351.2212354},
	abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses
              asynchronous, dynamic, graph-parallel
              computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees.
            
            We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
	language = {en},
	number = {8},
	urldate = {2025-04-11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
	month = apr,
	year = {2012},
	pages = {716--727},
}

@article{low_distributed_2012-1,
	title = {Distributed {GraphLab}: a framework for machine learning and data mining in the cloud},
	volume = {5},
	issn = {2150-8097},
	shorttitle = {Distributed {GraphLab}},
	url = {https://dl.acm.org/doi/10.14778/2212351.2212354},
	doi = {10.14778/2212351.2212354},
	abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees.We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
	number = {8},
	urldate = {2025-04-11},
	journal = {Proc. VLDB Endow.},
	author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
	month = apr,
	year = {2012},
	pages = {716--727},
}

@misc{noauthor_giraphproposal_nodate,
	title = {{GiraphProposal} - {INCUBATOR} - {Apache} {Software} {Foundation}},
	url = {https://cwiki.apache.org/confluence/display/INCUBATOR/GiraphProposal},
	urldate = {2025-04-11},
}

@article{chen_powerlyra_2018,
	title = {{PowerLyra}: {Differentiated} {Graph} {Computation} and {Partitioning} on {Skewed} {Graphs}},
	volume = {5},
	issn = {2329-4949, 2329-4957},
	shorttitle = {{PowerLyra}},
	url = {https://dl.acm.org/doi/10.1145/3298989},
	doi = {10.1145/3298989},
	abstract = {Natural graphs with skewed distributions raise unique challenges to distributed graph computation and partitioning. Existing graph-parallel systems usually use a “one-size-fits-all” design that uniformly processes all vertices, which either suffer from notable load imbalance and high contention for high-degree vertices (e.g., Pregel and GraphLab) or incur high communication cost and memory consumption even for low-degree vertices (e.g., PowerGraph and GraphX). In this article, we argue that skewed distributions in natural graphs also necessitate differentiated processing on high-degree and low-degree vertices. We then introduce PowerLyra, a new distributed graph processing system that embraces the best of both worlds of existing graph-parallel systems. Specifically, PowerLyra uses centralized computation for low-degree vertices to avoid frequent communications and distributes the computation for high-degree vertices to balance workloads. PowerLyra further provides an efficient hybrid graph partitioning algorithm (i.e., hybrid-cut) that combines edge-cut (for low-degree vertices) and vertex-cut (for high-degree vertices) with heuristics. To improve cache locality of inter-node graph accesses, PowerLyra further provides a locality-conscious data layout optimization. PowerLyra is implemented based on the latest GraphLab and can seamlessly support various graph algorithms running in both synchronous and asynchronous execution modes. A detailed evaluation on three clusters using various graph-analytics and MLDM (Machine Learning and Data Mining) applications shows that PowerLyra outperforms PowerGraph by up to 5.53X (from 1.24X) and 3.26X (from 1.49X) for real-world and synthetic graphs, respectively, and is much faster than other systems like GraphX and Giraph, yet with much less memory consumption. A porting of hybrid-cut to GraphX further confirms the efficiency and generality of PowerLyra.},
	language = {en},
	number = {3},
	urldate = {2025-04-07},
	journal = {ACM Transactions on Parallel Computing},
	author = {Chen, Rong and Shi, Jiaxin and Chen, Yanzhe and Zang, Binyu and Guan, Haibing and Chen, Haibo},
	month = sep,
	year = {2018},
	pages = {1--39},
}

@inproceedings{wu_approxrank_2009,
	title = {{ApproxRank}: {Estimating} {Rank} for a {Subgraph}},
	shorttitle = {{ApproxRank}},
	url = {https://ieeexplore.ieee.org/document/4812391},
	doi = {10.1109/ICDE.2009.108},
	abstract = {Customized semantic query answering, personalized search, focused crawlers and localized search engines frequently focus on ranking the pages contained within a subgraph of the global Web graph. The challenge for these applications is to compute PageRank-style scores efficiently on the subgraph, i.e., the ranking must reflect the global link structure of the Web graph but it must do so without paying the high overhead associated with a global computation. We propose a framework of an exact solution and an approximate solution for computing ranking on a subgraph. The IdealRank algorithm is an exact solution with the assumption that the scores of external pages are known. We prove that the IdealRank scores for pages in the subgraph converge. Since the PageRank-style scores of external pages may not typically be available, we propose the ApproxRank algorithm to estimate scores for the subgraph. Both IdealRank and ApproxRank represent the set of external pages with an external node L1 and extend the subgraph with links to L1. They also modify the PageRank-style transition matrix with respect to L1. We analyze the L1 distance between IdealRank scores and ApproxRank scores of the subgraph and show that it is within a constant factor of the L1 distance of the external pages (e.g., the true PageRank scores and uniform scores assumed by ApproxRank). We compare ApproxRank and a stochastic complementation approach (SC), a current best solution for this problem, on different types of subgraphs. ApproxRank has similar or superior performance to SC and typically improves on the runtime performance of SC by an order of magnitude or better. We demonstrate that ApproxRank provides a good approximation to PageRank for a variety of subgraphs.},
	urldate = {2025-04-07},
	booktitle = {2009 {IEEE} 25th {International} {Conference} on {Data} {Engineering}},
	author = {Wu, Yao and Raschid, Louiqa},
	month = mar,
	year = {2009},
	note = {ISSN: 2375-026X},
	keywords = {Application software, Computer applications, Crawlers, Data engineering, Educational institutions, Explosions, Runtime, Search engines, Stochastic processes, Web pages},
	pages = {54--65},
}

@article{liu_fast_2015,
	title = {Fast {PageRank} approximation by adaptive sampling},
	volume = {42},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-013-0691-1},
	doi = {10.1007/s10115-013-0691-1},
	abstract = {PageRank is typically computed from the power of transition matrix in a Markov Chain model. It is therefore computationally expensive, and efficient approximation methods to accelerate the computation are necessary, especially when it comes to large graphs. In this paper, we propose two sampling algorithms for PageRank efficient approximation: Direct sampling and Adaptive sampling. Both methods sample the transition matrix and use the sample in PageRank computation. Direct sampling method samples the transition matrix once and uses the sample directly in PageRank computation, whereas adaptive sampling method samples the transition matrix multiple times with an adaptive sample rate which is adjusted iteratively as the computing procedure proceeds. This adaptive sample rate is designed for a good trade-off between accuracy and efficiency for PageRank approximation. We provide detailed theoretical analysis on the error bounds of both methods. We also compare them with several state-of-the-art PageRank approximation methods, including power extrapolation and inner–outer power iteration algorithm. Experimental results on several real-world datasets show that our methods can achieve significantly higher efficiency while attaining comparable accuracy than state-of-the-art methods.},
	language = {en},
	number = {1},
	urldate = {2025-04-07},
	journal = {Knowledge and Information Systems},
	author = {Liu, Wenting and Li, Guangxia and Cheng, James},
	month = jan,
	year = {2015},
	keywords = {Adaptive Sampling, PageRank, Power iteration},
	pages = {127--146},
}

@inproceedings{tian_towards_2017,
	title = {Towards memory and computation efficient graph processing on spark},
	url = {https://ieeexplore.ieee.org/document/8257948},
	doi = {10.1109/BigData.2017.8257948},
	abstract = {Algorithms for large scale natural graph processing can be categorized into two types based on their value propagation behaviors: the unidirectional value propagation (UVP) algorithms and the bidirectional value propagation (BVP) algorithms. The behavior about how vertices interact with neighbors also differs between two algorithm types, which demands different system design choices. However, current distributed graph processing systems usually try to support both types in one general-purpose framework Such system design can not promise good performance and low resource consumption for both types. Especially, for UVP algorithms, current systems can not guarantee low memory footprint, computation efficiency and communication efficiency at the same time. In this paper, we propose a new graph processing engine on Spark, GraphV, which is specially designed for the unidirectional value propagation algorithms, and can satisfy all the above requirements for this type of algorithms. To retain the generalization for other algorithms, we also build a dual-engine framework by integrating GraphV with Spark's existing graph processing engine GraphX. The main design choices of GraphV include a cheap propagation-related partitioner, an one-step computation model, and a locality-aware local graph layout. According to the experiment results, GraphV is faster than GraphX by the factors of 1.2x-3.1x, with much less resource consumption. The source code of GraphV will be publicly available from http://prof.ict.ac.cn/GraphV.},
	urldate = {2025-04-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Tian, Xinhui and Guo, Yuanqing and Zhan, Jianfeng and Wang, Lei},
	month = dec,
	year = {2017},
	keywords = {Algorithm design and analysis, Computational modeling, Engines, Mirrors, Partitioning algorithms, Spark, Sparks, Synchronization, big graph, distributed system},
	pages = {375--382},
}

@inproceedings{bhattacharya_comparative_2024,
	title = {Comparative {Study} of {Large} {Graph} {Processing} {Systems}},
	url = {https://ieeexplore.ieee.org/document/10914110},
	doi = {10.1109/CALCON63337.2024.10914110},
	abstract = {In the modern world, all real-life problems, such as road networks, telecommunication networks, recommendation systems, social network interactions and so on can be modeled using Graph Data Structure. Over time, the data accumulates for each of these, and as a result, the size of the graph also increases. The large graphs highlight some constraints of the computational systems with respect to processing speed, accuracy, and memory usage. In the last few decades, there have been extensive research and results on optimizing large graph processing for efficient storage and quicker results. This paper provides an extensive comparative survey of the state-of-the-art centralised and distributed approaches that are utilised for large graph processing with respect to processing speed and resource utilization on both on-premise as well as cloud architecture. There is focus on both theoretical as well empirical analysis, highlighting some of the applications of existing large graph processing systems as well. This paper also provides a taxonomy of the large graph processing frameworks/tools covering all popular and relevant categories and sub-categories.},
	urldate = {2025-04-07},
	booktitle = {2024 {IEEE} {Calcutta} {Conference} ({CALCON})},
	author = {Bhattacharya, Subhayan and Ghosal, Rohit and Roy, Sarbani},
	month = dec,
	year = {2024},
	keywords = {Cloud Computing, Cloud computing, Data models, Data structures, Distributed Computing, Large Graph Processing, Recommender systems, Resource management, Roads, Social networking (online), Stress, Surveys, Taxonomy},
	pages = {1--6},
}

@misc{page_pagerank_1999,
	type = {Techreport},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}.},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {http://ilpubs.stanford.edu:8090/422/?utm_campaign=Technical%20SEO%20Weekly&utm_medium=email&utm_source=Revue%20newsletter},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2025-04-04},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	note = {Publisher: Stanford InfoLab},
}

@article{zhu_gemini_nodate,
	title = {Gemini: {A} {Computation}-{Centric} {Distributed} {Graph} {Processing} {System}},
	abstract = {Traditionally distributed graph processing systems have largely focused on scalability through the optimizations of inter-node communication and load balance. However, they often deliver unsatisfactory overall processing efﬁciency compared with shared-memory graph computing frameworks. We analyze the behavior of several graph-parallel systems and ﬁnd that the added overhead for achieving scalability becomes a major limiting factor for efﬁciency, especially with modern multi-core processors and high-speed interconnection networks.},
	language = {en},
	author = {Zhu, Xiaowei and Chen, Wenguang and Zheng, Weimin and Ma, Xiaosong},
}

@misc{xin_graphx_2014,
	title = {{GraphX}: {Unifying} {Data}-{Parallel} and {Graph}-{Parallel} {Analytics}},
	shorttitle = {{GraphX}},
	url = {http://arxiv.org/abs/1402.2394},
	doi = {10.48550/arXiv.1402.2394},
	abstract = {From social networks to language modeling, the growing scale and importance of graph data has driven the development of numerous new graph-parallel systems (e.g., Pregel, GraphLab). By restricting the computation that can be expressed and introducing new techniques to partition and distribute the graph, these systems can efficiently execute iterative graph algorithms orders of magnitude faster than more general data-parallel systems. However, the same restrictions that enable the performance gains also make it difficult to express many of the important stages in a typical graph-analytics pipeline: constructing the graph, modifying its structure, or expressing computation that spans multiple graphs. As a consequence, existing graph analytics pipelines compose graph-parallel and data-parallel systems using external storage systems, leading to extensive data movement and complicated programming model. To address these challenges we introduce GraphX, a distributed graph computation framework that unifies graph-parallel and data-parallel computation. GraphX provides a small, core set of graph-parallel operators expressive enough to implement the Pregel and PowerGraph abstractions, yet simple enough to be cast in relational algebra. GraphX uses a collection of query optimization techniques such as automatic join rewrites to efficiently implement these graph-parallel operators. We evaluate GraphX on real-world graphs and workloads and demonstrate that GraphX achieves comparable performance as specialized graph computation systems, while outperforming them in end-to-end graph pipelines. Moreover, GraphX achieves a balance between expressiveness, performance, and ease of use.},
	urldate = {2025-04-04},
	publisher = {arXiv},
	author = {Xin, Reynold S. and Crankshaw, Daniel and Dave, Ankur and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion},
	month = feb,
	year = {2014},
	note = {arXiv:1402.2394 [cs]},
	keywords = {Computer Science - Databases},
}

@misc{noauthor_fast_nodate,
	title = {Fast {PageRank} approximation by adaptive sampling {\textbar} {Knowledge} and {Information} {Systems}},
	url = {https://dl.acm.org/doi/10.1007/s10115-013-0691-1},
	urldate = {2025-04-04},
}

@misc{noauthor_local_nodate,
	title = {Local approximation of pagerank and reverse pagerank {\textbar} {Proceedings} of the 17th {ACM} conference on {Information} and knowledge management},
	url = {https://dl.acm.org/doi/10.1145/1458082.1458122},
	urldate = {2025-04-04},
}

@inproceedings{wu_approxrank_2009-1,
	title = {{ApproxRank}: {Estimating} {Rank} for a {Subgraph}},
	shorttitle = {{ApproxRank}},
	url = {https://ieeexplore.ieee.org/document/4812391},
	doi = {10.1109/ICDE.2009.108},
	abstract = {Customized semantic query answering, personalized search, focused crawlers and localized search engines frequently focus on ranking the pages contained within a subgraph of the global Web graph. The challenge for these applications is to compute PageRank-style scores efficiently on the subgraph, i.e., the ranking must reflect the global link structure of the Web graph but it must do so without paying the high overhead associated with a global computation. We propose a framework of an exact solution and an approximate solution for computing ranking on a subgraph. The IdealRank algorithm is an exact solution with the assumption that the scores of external pages are known. We prove that the IdealRank scores for pages in the subgraph converge. Since the PageRank-style scores of external pages may not typically be available, we propose the ApproxRank algorithm to estimate scores for the subgraph. Both IdealRank and ApproxRank represent the set of external pages with an external node L1 and extend the subgraph with links to L1. They also modify the PageRank-style transition matrix with respect to L1. We analyze the L1 distance between IdealRank scores and ApproxRank scores of the subgraph and show that it is within a constant factor of the L1 distance of the external pages (e.g., the true PageRank scores and uniform scores assumed by ApproxRank). We compare ApproxRank and a stochastic complementation approach (SC), a current best solution for this problem, on different types of subgraphs. ApproxRank has similar or superior performance to SC and typically improves on the runtime performance of SC by an order of magnitude or better. We demonstrate that ApproxRank provides a good approximation to PageRank for a variety of subgraphs.},
	urldate = {2025-04-04},
	booktitle = {2009 {IEEE} 25th {International} {Conference} on {Data} {Engineering}},
	author = {Wu, Yao and Raschid, Louiqa},
	month = mar,
	year = {2009},
	note = {ISSN: 2375-026X},
	keywords = {Application software, Computer applications, Crawlers, Data engineering, Educational institutions, Explosions, Runtime, Search engines, Stochastic processes, Web pages},
	pages = {54--65},
}

@article{avrachenkov_monte_2007,
	title = {Monte {Carlo} {Methods} in {Pagerank} {Computation}: {When} {One} {Iteration} {Is} {Sufficient}},
	volume = {45},
	issn = {0036-1429},
	shorttitle = {Monte {Carlo} {Methods} in {Pagerank} {Computation}},
	url = {https://www.jstor.org/stable/40232890},
	abstract = {PageRank is one of the principle criteria according to which Google ranks Web pages. PageRank can be interpreted as a frequency of visiting a Web page by a random surfer, and thus it reflects the popularity of a Web page. Google computes the PageRank using the power iteration method, which requires about one week of intensive computations. In the present work we propose and analyze Monte Carlo-type methods for the PageRank computation. There are several advantages of the probabilistic Monte Carlo methods over the deterministic power iteration method: Monte Carlo methods already provide good estimation of the PageRank for relatively important pages after one iteration; Monte Carlo methods have natural parallel implementation; and finally, Monte Carlo methods allow one to perform continuous update of the PageRank as the structure of the Web changes.},
	number = {2},
	urldate = {2025-04-04},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Avrachenkov, K. and Litvak, N. and Nemirovsky, D. and Osipova, N.},
	year = {2007},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {890--904},
}

@article{noauthor_pdf_2024,
	title = {({PDF}) {Monte} {Carlo} {Methods} in {PageRank} {Computation}: {When} {One} {Iteration} is {Sufficient}},
	shorttitle = {({PDF}) {Monte} {Carlo} {Methods} in {PageRank} {Computation}},
	url = {https://www.researchgate.net/publication/220179809_Monte_Carlo_Methods_in_PageRank_Computation_When_One_Iteration_is_Sufficient},
	doi = {10.1137/050643799},
	abstract = {PDF {\textbar} PageRank is one of the principle criteria according to which Google ranks Web pages. PageRank can be interpreted as a frequency of vis-iting a Web... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-04-04},
	journal = {ResearchGate},
	month = oct,
	year = {2024},
}

@article{xie_parameterized_2023,
	title = {A {Parameterized} {Multi}-{Splitting} {Iterative} {Method} for {Solving} the {PageRank} {Problem}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/15/3320},
	doi = {10.3390/math11153320},
	abstract = {In this paper, a new multi-parameter iterative algorithm is proposed to address the PageRank problem based on the multi-splitting iteration method. The proposed method solves two linear subsystems at each iteration by splitting the coefficient matrix, considering therefore inner and outer iteration to find the approximate solutions of these linear subsystems. It can be shown that the iterative sequence generated by the multi-parameter iterative algorithm finally converges to the PageRank vector when the parameters satisfy certain conditions. Numerical experiments show that the proposed algorithm has better convergence and numerical stability than the existing algorithms.},
	language = {en},
	number = {15},
	urldate = {2025-04-03},
	journal = {Mathematics},
	author = {Xie, Yajun and Hu, Lihua and Ma, Changfeng},
	month = jan,
	year = {2023},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {PageRank, inner subsystems, inner–outer iterations, multi-parameter iteration},
	pages = {3320},
}

@misc{noauthor_graphx_nodate,
	title = {{GraphX} - {Spark} 3.5.5 {Documentation}},
	url = {https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api},
	urldate = {2025-04-03},
}

@inproceedings{xin_graphx_2013,
	address = {New York New York},
	title = {{GraphX}: a resilient distributed graph system on {Spark}},
	isbn = {978-1-4503-2188-4},
	shorttitle = {{GraphX}},
	url = {https://dl.acm.org/doi/10.1145/2484425.2484427},
	doi = {10.1145/2484425.2484427},
	language = {en},
	urldate = {2025-04-03},
	booktitle = {First {International} {Workshop} on {Graph} {Data} {Management} {Experiences} and {Systems}},
	publisher = {ACM},
	author = {Xin, Reynold S. and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion},
	month = jun,
	year = {2013},
	pages = {1--6},
}

@article{yang_efficient_2024,
	title = {Efficient {Algorithms} for {Personalized} {PageRank} {Computation}: {A} {Survey}},
	volume = {36},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Efficient {Algorithms} for {Personalized} {PageRank} {Computation}},
	url = {http://arxiv.org/abs/2403.05198},
	doi = {10.1109/TKDE.2024.3376000},
	abstract = {Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes \$s\$ and \$t\$, the PPR value \${\textbackslash}pi\_s(t)\$ equals the probability that an \${\textbackslash}alpha\$-discounted random walk from \$s\$ terminates at \$t\$ and reflects the importance between \$s\$ and \$t\$ in a bidirectional way. As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.},
	number = {9},
	urldate = {2025-04-03},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Mingji and Wang, Hanzhi and Wei, Zhewei and Wang, Sibo and Wen, Ji-Rong},
	month = sep,
	year = {2024},
	note = {arXiv:2403.05198 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	pages = {4582--4602},
}

@article{jayaram_dynamic_2024,
	title = {Dynamic {PageRank}: {Algorithms} and {Lower} {Bounds}},
	volume = {297},
	copyright = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/openAccess},
	issn = {1868-8969},
	shorttitle = {Dynamic {PageRank}},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ICALP.2024.90},
	doi = {10.4230/LIPICS.ICALP.2024.90},
	abstract = {We consider the PageRank problem in the dynamic setting, where the goal is to explicitly maintain an approximate PageRank vector π ∈ Rn for a graph under a sequence of edge insertions and deletions. Our main result is a complete characterization of the complexity of dynamic PageRank maintenance for both multiplicative and additive (L1) approximations.},
	language = {en},
	urldate = {2025-04-03},
	journal = {LIPIcs, Volume 297, ICALP 2024},
	author = {Jayaram, Rajesh and Łącki, Jakub and Mitrović, Slobodan and Onak, Krzysztof and Sankowski, Piotr},
	editor = {Bringmann, Karl and Grohe, Martin and Puppis, Gabriele and Svensson, Ola},
	year = {2024},
	note = {Artwork Size: 19 pages, 898032 bytes
ISBN: 9783959773225
Medium: application/pdf
Publisher: Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	keywords = {Information systems → Page and site ranking, Mathematics of computing → Graph algorithms, PageRank, Theory of computation → Dynamic graph algorithms, Theory of computation → Random walks and Markov chains, dynamic algorithms, graph algorithms},
	pages = {90:1--90:19},
}

@article{gonzalez_graphx_nodate,
	title = {{GraphX}: {Graph} {Processing} in a {Distributed} {Dataﬂow} {Framework}},
	abstract = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataﬂow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataﬂow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataﬂow system. GraphX presents a familiar composable graph abstraction that is sufﬁcient to express existing graph APIs, yet can be implemented using only a few basic dataﬂow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-speciﬁc optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataﬂow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataﬂow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.},
	language = {en},
	author = {Gonzalez, Joseph E and Xin, Reynold S and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J and Stoica, Ion},
}

@inproceedings{shanahan_large_2015,
	address = {New York, NY, USA},
	series = {{KDD} '15},
	title = {Large {Scale} {Distributed} {Data} {Science} using {Apache} {Spark}},
	isbn = {978-1-4503-3664-2},
	url = {https://dl.acm.org/doi/10.1145/2783258.2789993},
	doi = {10.1145/2783258.2789993},
	abstract = {Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.},
	urldate = {2025-04-03},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Shanahan, James G. and Dai, Laing},
	month = aug,
	year = {2015},
	pages = {2323--2324},
}

@article{langville_googles_nodate,
	title = {Google's {PageRank} and {Beyond}},
	language = {en},
	author = {Langville, Amy N and Meyer, Carl D},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {A} {Parameterized} {Multisplitting} {Iterative} {Method} for {Solving} the {PageRank} {Problem}},
	url = {https://www.researchgate.net/publication/371913713_A_Parameterized_Multisplitting_Iterative_Method_for_Solving_the_PageRank_Problem},
	abstract = {PDF {\textbar} In this paper, a new multi parameter iterative algorithm is proposed to address the PageRank problem based on the multi-splitting iteration method... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-03-28},
	journal = {ResearchGate},
	doi = {10.20944/preprints202306.1917.v1},
}

@article{anikin_efficient_2022,
	title = {Efficient {Numerical} {Methods} to {Solve} {Sparse} {Linear} {Equations} with {Application} to {PageRank}},
	volume = {37},
	issn = {1055-6788, 1029-4937},
	url = {http://arxiv.org/abs/1508.07607},
	doi = {10.1080/10556788.2020.1858297},
	abstract = {In this paper, we propose three methods to solve the PageRank problem for the transition matrices with both row and column sparsity. Our methods reduce the PageRank problem to the convex optimization problem over the simplex. The first algorithm is based on the gradient descent in L1 norm instead of the Euclidean one. The second algorithm extends the Frank-Wolfe to support sparse gradient updates. The third algorithm stands for the mirror descent algorithm with a randomized projection. We proof converges rates for these methods for sparse problems as well as numerical experiments support their effectiveness.},
	number = {3},
	urldate = {2025-03-28},
	journal = {Optimization Methods and Software},
	author = {Anikin, Anton and Gasnikov, Alexander and Gornov, Alexander and Kamzolov, Dmitry and Maximov, Yury and Nesterov, Yurii},
	month = may,
	year = {2022},
	note = {arXiv:1508.07607 [math]},
	keywords = {Mathematics - Optimization and Control},
	pages = {907--935},
}

@article{bahmani_fast_2010,
	title = {Fast incremental and personalized {PageRank}},
	volume = {4},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1929861.1929864},
	doi = {10.14778/1929861.1929864},
	abstract = {In this paper, we analyze the efficiency of Monte Carlo methods for incremental computation of PageRank, personalized PageRank, and similar random walk based methods (with focus on SALSA), on large-scale dynamically evolving social networks. We assume that the graph of friendships is stored in distributed shared memory, as is the case for large social networks such as Twitter.For global PageRank, we assume that the social network has n nodes, and m adversarially chosen edges arrive in a random order. We show that with a reset probability of ε, the expected total work needed to maintain an accurate estimate (using the Monte Carlo method) of the PageRank of every node at all times is [EQUATION]. This is significantly better than all known bounds for incremental PageRank. For instance, if we naively recompute the PageRanks as each edge arrives, the simple power iteration method needs [EQUATION] total time and the Monte Carlo method needs O(mn/ε) total time; both are prohibitively expensive. We also show that we can handle deletions equally efficiently.We then study the computation of the top k personalized PageRanks starting from a seed node, assuming that personalized PageRanks follow a power-law with exponent α \&lt; 1. We show that if we store R \&gt; q ln n random walks starting from every node for large enough constant q (using the approach outlined for global PageRank), then the expected number of calls made to the distributed social network database is O(k/(R(1-α)/α)). We also present experimental results from the social networking site, Twitter, verifying our assumptions and analyses. The overall result is that this algorithm is fast enough for real-time queries over a dynamic social network.},
	number = {3},
	urldate = {2025-03-24},
	journal = {Proc. VLDB Endow.},
	author = {Bahmani, Bahman and Chowdhury, Abdur and Goel, Ashish},
	year = {2010},
	pages = {173--184},
}

@article{hou_personalized_2023,
	title = {Personalized {PageRank} on {Evolving} {Graphs} with an {Incremental} {Index}-{Update} {Scheme}},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3588705},
	doi = {10.1145/3588705},
	abstract = {{\textbackslash}em Personalized PageRank (PPR) stands as a fundamental proximity measure in graph mining. Given an input graph G with the probability of decay α, a source node s and a target node t, the PPR score π(s,t) of target t with respect to source s is the probability that an α-decay random walk starting from s stops at t. A {\textbackslash}em single-source PPR (SSPPR) query takes an input graph G with decay probability α and a source s, and then returns the PPR π(s,v) for each node v ∈ V. Since computing an exact SSPPR query answer is prohibitive, most existing solutions turn to approximate queries with guarantees. The state-of-the-art solutions for approximate SSPPR queries are index-based and mainly focus on static graphs, while real-world graphs are usually dynamically changing. However, existing index-update schemes can not achieve a sub-linear update time. Motivated by this, we present an efficient indexing scheme for single-source PPR queries on evolving graphs. Our proposed solution is based on a classic framework that combines the forward-push technique with a random walk index for approximate PPR queries. Thus, our indexing scheme is similar to existing solutions in the sense that we store pre-sampled random walks for efficient query processing. One of our main contributions is an incremental updating scheme to maintain indexed random walks in expected O(1) time after each graph update. To achieve O(1) update cost, we need to maintain auxiliary data structures for both vertices and edges. To reduce the space consumption, we further revisit the sampling methods and propose a new sampling scheme to remove the auxiliary data structure for vertices while still supporting O(1) index update cost on evolving graphs. Extensive experiments show that our update scheme achieves orders of magnitude speed-up on update performance over existing index-based dynamic schemes without sacrificing the query efficiency.},
	number = {1},
	urldate = {2025-03-24},
	journal = {Proc. ACM Manag. Data},
	author = {Hou, Guanhao and Guo, Qintian and Zhang, Fangyuan and Wang, Sibo and Wei, Zhewei},
	year = {2023},
	pages = {25:1--25:26},
}

@article{wu_efficient_2024,
	title = {Efficient and {Accurate} {PageRank} {Approximation} on {Large} {Graphs}},
	volume = {2},
	url = {https://dl.acm.org/doi/10.1145/3677132},
	doi = {10.1145/3677132},
	abstract = {PageRank is a commonly used measurement in a wide range of applications, including search engines, recommendation systems, and social networks. However, this measurement suffers from huge computational overhead, which cannot be scaled to large graphs. Although many approximate algorithms have been proposed for computing PageRank values, these algorithms are either (i) not efficient or (ii) not accurate. Worse still, some of them cannot provide estimated PageRank values for all the vertices. In this paper, we first propose the CUR-Trans algorithm, which can reduce the time complexity for computing PageRank values and has lower error bound than existing matrix approximation-based PageRank algorithms. Then, we develop the T 2-Approx algorithm to further reduce the time complexity for computing this measurement. Experiment results on three large-scale graphs show that both the CUR-Trans algorithm and the T 2-Approx algorithm achieve the lowest response time for computing PageRank values with the best accuracy (for the CUR-Trans algorithm) or the competitive accuracy (for the T 2-Approx algorithm). Besides, the two proposed algorithms are able to provide estimated PageRank values for all the vertices.},
	number = {4},
	urldate = {2025-03-21},
	journal = {Proc. ACM Manag. Data},
	author = {Wu, Siyue and Wu, Dingming and Quan, Junyi and Chan, Tsz Nam and Lu, Kezhong},
	month = sep,
	year = {2024},
	pages = {196:1--196:26},
}

@article{zhuo_distributed_2021,
	title = {Distributed {Graph} {Processing} {System} and {Processing}-in-memory {Architecture} with {Precise} {Loop}-carried {Dependency} {Guarantee}},
	volume = {37},
	issn = {0734-2071},
	url = {https://dl.acm.org/doi/10.1145/3453681},
	doi = {10.1145/3453681},
	abstract = {To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—GraphS and GraphSR—for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.},
	number = {1-4},
	urldate = {2025-03-17},
	journal = {ACM Trans. Comput. Syst.},
	author = {Zhuo, Youwei and Chen, Jingji and Rao, Gengyu and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
	month = jul,
	year = {2021},
	pages = {5:1--5:37},
}

@misc{noauthor_sync_nodate,
	title = {{SYNC} or {ASYNC}: time to fuse for distributed graph-parallel computation {\textbar} {Proceedings} of the 20th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	url = {https://dl.acm.org/doi/10.1145/2688500.2688508},
	urldate = {2025-03-17},
}

@misc{noauthor_chaos_nodate,
	title = {Chaos {\textbar} {Proceedings} of the 25th {Symposium} on {Operating} {Systems} {Principles}},
	url = {https://dl.acm.org/doi/10.1145/2815400.2815408},
	urldate = {2025-03-17},
}

@article{gong_automating_2021,
	title = {Automating incremental graph processing with flexible memoization},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3461535.3461550},
	doi = {10.14778/3461535.3461550},
	abstract = {The ever-growing amount of dynamic graph data demands efficient techniques of incremental graph processing. However, incremental graph algorithms are challenging to develop. Existing approaches usually require users to manually design nontrivial incremental operators, or choose different memoization strategies for certain specific types of computation, limiting the usability and generality.In light of these challenges, we propose Ingress, an automated system for \&lt;u\&gt;in\&lt;/u\&gt;cremental \&lt;u\&gt;g\&lt;/u\&gt;raph proc\&lt;u\&gt;ess\&lt;/u\&gt;ing. Ingress is able to incrementalize batch vertex-centric algorithms into their incremental counterparts as a whole, without the need of redesigned logic or data structures from users. Underlying Ingress is an automated incrementalization framework equipped with four different memoization policies, to support all kinds of vertex-centric computations with optimized memory utilization. We identify sufficient conditions for the applicability of these policies. Ingress chooses the best-fit policy for a given algorithm automatically by verifying these conditions. In addition to the ease-of-use and generalization, Ingress outperforms state-of-the-art incremental graph systems by 15.93X on average (up to 147.14X) in efficiency.},
	number = {9},
	urldate = {2025-03-17},
	journal = {Proc. VLDB Endow.},
	author = {Gong, Shufeng and Tian, Chao and Yin, Qiang and Yu, Wenyuan and Zhang, Yanfeng and Geng, Liang and Yu, Song and Yu, Ge and Zhou, Jingren},
	year = {2021},
	pages = {1613--1625},
}

@inproceedings{zhu_gemini_2016,
	address = {USA},
	series = {{OSDI}'16},
	title = {Gemini: a computation-centric distributed graph processing system},
	isbn = {978-1-931971-33-1},
	shorttitle = {Gemini},
	abstract = {Traditionally distributed graph processing systems have largely focused on scalability through the optimizations of inter-node communication and load balance. However, they often deliver unsatisfactory overall processing efficiency compared with shared-memory graph computing frameworks. We analyze the behavior of several graph-parallel systems and find that the added overhead for achieving scalability becomes a major limiting factor for efficiency, especially with modern multi-core processors and high-speed interconnection networks.Based on our observations, we present Gemini, a distributed graph processing system that applies multiple optimizations targeting computation performance to build scalability on top of efficiency. Gemini adopts (1) a sparse-dense signal-slot abstraction to extend the hybrid push-pull computation model from shared-memory to distributed scenarios, (2) a chunk-based partitioning scheme enabling low-overhead scaling out designs and locality-preserving vertex accesses, (3) a dual representation scheme to compress accesses to vertex indices, (4) NUMA-aware sub-partitioning for efficient intra-node memory accesses, plus (5) locality-aware chunking and fine-grained work-stealing for improving both internode and intra-node load balance, respectively. Our evaluation on an 8-node high-performance cluster (using five widely used graph applications and five real-world graphs) shows that Gemini significantly outperforms all well-known existing distributed graph processing systems, delivering up to 39.8× (from 8.91×) improvement over the fastest among them.},
	urldate = {2025-03-17},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Zhu, Xiaowei and Chen, Wenguang and Zheng, Weimin and Ma, Xiaosong},
	month = nov,
	year = {2016},
	pages = {301--316},
}

@inproceedings{gonzalez_powergraph_2012,
	address = {USA},
	series = {{OSDI}'12},
	title = {{PowerGraph}: distributed graph-parallel computation on natural graphs},
	isbn = {978-1-931971-96-6},
	shorttitle = {{PowerGraph}},
	abstract = {Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability.In this paper, we characterize the challenges of computation on natural graphs in the context of existing graph-parallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.},
	urldate = {2025-03-17},
	booktitle = {Proceedings of the 10th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Gonzalez, Joseph E. and Low, Yucheng and Gu, Haijie and Bickson, Danny and Guestrin, Carlos},
	year = {2012},
	pages = {17--30},
}

@misc{noauthor_software_nodate,
	title = {Software {Systems} {Implementation} and {Domain}-{Specific} {Architectures} towards {Graph} {Analytics}},
	url = {https://spj.science.org/doi/10.34133/2022/9806758},
	language = {en},
	urldate = {2025-03-17},
	doi = {10.34133/2022/9806758},
}

@article{gong_automating_2021-1,
	title = {Automating incremental graph processing with flexible memoization},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3461535.3461550},
	doi = {10.14778/3461535.3461550},
	abstract = {The ever-growing amount of dynamic graph data demands efficient techniques of incremental graph processing. However, incremental graph algorithms are challenging to develop. Existing approaches usually require users to manually design nontrivial incremental operators, or choose different memoization strategies for certain specific types of computation, limiting the usability and generality.In light of these challenges, we propose Ingress, an automated system for \&lt;u\&gt;in\&lt;/u\&gt;cremental \&lt;u\&gt;g\&lt;/u\&gt;raph proc\&lt;u\&gt;ess\&lt;/u\&gt;ing. Ingress is able to incrementalize batch vertex-centric algorithms into their incremental counterparts as a whole, without the need of redesigned logic or data structures from users. Underlying Ingress is an automated incrementalization framework equipped with four different memoization policies, to support all kinds of vertex-centric computations with optimized memory utilization. We identify sufficient conditions for the applicability of these policies. Ingress chooses the best-fit policy for a given algorithm automatically by verifying these conditions. In addition to the ease-of-use and generalization, Ingress outperforms state-of-the-art incremental graph systems by 15.93X on average (up to 147.14X) in efficiency.},
	number = {9},
	urldate = {2025-03-15},
	journal = {Proc. VLDB Endow.},
	author = {Gong, Shufeng and Tian, Chao and Yin, Qiang and Yu, Wenyuan and Zhang, Yanfeng and Geng, Liang and Yu, Song and Yu, Ge and Zhou, Jingren},
	year = {2021},
	pages = {1613--1625},
}

@article{qian_graph_2021,
	title = {Graph processing and machine learning architectures with emerging memory technologies: a survey},
	volume = {64},
	issn = {1869-1919},
	shorttitle = {Graph processing and machine learning architectures with emerging memory technologies},
	url = {https://doi.org/10.1007/s11432-020-3219-6},
	doi = {10.1007/s11432-020-3219-6},
	abstract = {This paper surveys domain-specific architectures (DSAs) built from two emerging memory technologies. Hybrid memory cube (HMC) and high bandwidth memory (HBM) can reduce data movement between memory and computation by placing computing logic inside memory dies. On the other hand, the emerging non-volatile memory, metal-oxide resistive random access memory (ReRAM) has been considered as a promising candidate for future memory architecture due to its high density, fast read access and low leakage power. The key feature is ReRAM’s capability to perform the inherently parallel in-situ matrix-vector multiplication in the analog domain. We focus on the DSAs for two important applications—graph processing and machine learning acceleration. Based on the understanding of the recent architectures and our research experience, we also discuss several potential research directions.},
	language = {en},
	number = {6},
	urldate = {2025-03-14},
	journal = {Science China Information Sciences},
	author = {Qian, Xuehai},
	month = may,
	year = {2021},
	keywords = {HMC/HBM, ReRAM, graph processing, machine learning acceleration},
	pages = {160401},
}

@article{hu_edge_2023,
	title = {An edge re-ordering based acceleration architecture for improving data locality in graph analytics applications},
	volume = {102},
	issn = {0141-9331},
	url = {https://www.sciencedirect.com/science/article/pii/S0141933123001394},
	doi = {10.1016/j.micpro.2023.104895},
	abstract = {Data structure is the key in Edge Computing where various types of data are continuously generated by ubiquitous devices. Within all common data structures, graphs are used to express relationships and dependencies among human identities, objects, and locations. They are also expected to become one of the most important data infrastructures in the near future. Furthermore, as graph processing often requires random accesses to vast memory spaces, conventional memory hierarchies with caches cannot work efficiently. To alleviate such memory access bottlenecks in graph processing, we present a solution through vertex accesses scheduling and edge array re-ordering, in parallel with the execution of graph processing applications to improve both temporal and spatial locality of memory accesses, especially for edge-centric graph analytics which are popular means in handling dynamic graphs. Our proposed architecture is evaluated and tested through both trace-based cache simulations and cycle-accurate FPGA-based prototyping. Evaluation results show that our proposal has a potential of significantly reducing the Last Level Cache (LLC) misses by 62.60\% in general among PageRank and BFS algorithms. Meanwhile, evaluations with the FPGA prototype successfully reduce the quantity of Miss-Per-Kilo-Instructions (MPKI) for LLC by 56.27\% on average.},
	urldate = {2025-03-14},
	journal = {Microprocessors and Microsystems},
	author = {Hu, Siyi and Kondo, Masaaki and He, Yuan and Sakamoto, Ryuichi and Zhang, Hao and Zhou, Jun and Nakamura, Hiroshi},
	month = oct,
	year = {2023},
	keywords = {Cache, Data locality, Domain-specific acceleration, Graph processing},
	pages = {104895},
}

@inproceedings{su_graflex_2024,
	address = {New York, NY, USA},
	series = {{FPGA} '24},
	title = {{GraFlex}: {Flexible} {Graph} {Processing} on {FPGAs} through {Customized} {Scalable} {Interconnection} {Network}},
	isbn = {9798400704185},
	shorttitle = {{GraFlex}},
	url = {https://doi.org/10.1145/3626202.3637573},
	doi = {10.1145/3626202.3637573},
	abstract = {Graph processing system design has been widely considered to be a challenging topic due to the mismatch between the computational throughput requirement and the memory bandwidth. Recent works try to deliver better graph processing systems by taking advantage of application-specific architectures and emerging high-bandwidth memory on FPGAs. However, there is still ample room for improvements regarding flexibility, scalability, and usability. This paper presents GraFlex, a flexible scatter-gather graph processing framework on FPGAs with scalable interconnection networks. It adopts the Bulk-Synchronous Parallel (BSP) paradigm for global control and synchronization, enabling rapid deployment of performant graph processing systems through HLS-based design flows. GraFlex conducts software-hardware co-optimization to boost system performance. It configures the compact graph format, partition scheme, and memory channel allocation strategy to support scalable designs. Resource-efficient multi-stage butterfly interconnection network achieves on-device data communication and facilitates throughput matching. To handle fragmented memory requests, we propose coalesced memory access engines to improve bandwidth utilization. GraFlex is comprehensively evaluated with various graph applications and real-world datasets. Our results show up to 2.09{\textbackslash}texttimes average speedup in traversal throughput over the existing state-of-the-art work with a non-negligible reduction in power and resource consumption. A case study of the breadth-first search (BFS) application shows a 6.58{\textbackslash}texttimes speedup in average algorithm throughout with proper implementation choices enabled by the scatter-gather mechanism implemented. The BFS study also reports an almost linear throughput scaling versus the number of processing elements (PEs) and memory channels.},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the 2024 {ACM}/{SIGDA} {International} {Symposium} on {Field} {Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Su, Chunyou and Du, Linfeng and Liang, Tingyuan and Lin, Zhe and Wang, Maolin and Sinha, Sharad and Zhang, Wei},
	month = apr,
	year = {2024},
	pages = {143--153},
}

@article{yu_multipim_2021,
	title = {{MultiPIM}: {A} {Detailed} and {Configurable} {Multi}-{Stack} {Processing}-{In}-{Memory} {Simulator}},
	volume = {20},
	issn = {1556-6064},
	shorttitle = {{MultiPIM}},
	url = {https://ieeexplore.ieee.org/document/9362242},
	doi = {10.1109/LCA.2021.3061905},
	abstract = {Processing-in-Memory (PIM) has being actively studied as a promising solution to overcome the memory wall problem. Therefore, there is an urgent need for a PIM simulation infrastructure to help researchers quickly understand existing problems and verify new mechanisms. However, existing PIM simulators do not consider architectural details and the programming interface that are necessary for a practical PIM system. In this letter, we present MultiPIM, a PIM simulator that models microarchitectural details that stem from supporting multiple memory stacks and massively-parallel PIM cores. On top of the detailed simulation infrastructure, MultiPIM provides an easy-to-use interface for configuring PIM hardware and adapting existing workloads for PIM offloading.},
	number = {1},
	urldate = {2025-03-14},
	journal = {IEEE Computer Architecture Letters},
	author = {Yu, Chao and Liu, Sihang and Khan, Samira},
	month = jan,
	year = {2021},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Control systems, Memory, Processing-in-memory, Random access memory, Switches, Task analysis, Timing, Topology, memory network, simulator},
	pages = {54--57},
}

@inproceedings{ham_graphicionado_2016,
	title = {Graphicionado: {A} high-performance and energy-efficient accelerator for graph analytics},
	shorttitle = {Graphicionado},
	url = {https://ieeexplore.ieee.org/document/7783759},
	doi = {10.1109/MICRO.2016.7783759},
	abstract = {Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76–6.54x speedup while consuming 50–100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.},
	urldate = {2025-03-13},
	booktitle = {2016 49th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Ham, Tae Jun and Wu, Lisa and Sundaram, Narayanan and Satish, Nadathur and Martonosi, Margaret},
	month = oct,
	year = {2016},
	keywords = {Machine learning algorithms, Memory management, Pipelines, Programming, Software, Software algorithms, System-on-chip},
	pages = {1--13},
}

@inproceedings{zhang_graphp_2018,
	title = {{GraphP}: {Reducing} {Communication} for {PIM}-{Based} {Graph} {Processing} with {Efficient} {Data} {Partition}},
	shorttitle = {{GraphP}},
	url = {https://ieeexplore.ieee.org/document/8327036},
	doi = {10.1109/HPCA.2018.00053},
	abstract = {Processing-In-Memory (PIM) is an effective technique that reduces data movements by integrating processing units within memory. The recent advance of “big data” and 3D stacking technology make PIM a practical and viable solution for the modern data processing workloads. It is exemplified by the recent research interests on PIM-based acceleration. Among them, TESSERACT is a PIM-enabled parallel graph processing architecture based on Micron’s Hybrid Memory Cube (HMC), one of the most prominent 3D-stacked memory technologies. It implements a Pregel-like vertex-centric programming model, so that users could develop programs in the familiar interface while taking advantage of PIM. Despite the orders of magnitude speedup compared to DRAM-based systems, TESSERACT generates excessive crosscube communications through SerDes links, whose bandwidth is much less than the aggregated local bandwidth of HMCs. Our investigation indicates that this is because of the restricted data organization required by the vertex programming model. In this paper, we argue that a PIM-based graph processing system should take data organization as a first-order design consideration. Following this principle, we propose GraphP, a novel HMC-based software/hardware co-designed graph processing system that drastically reduces communication and energy consumption compared to TESSERACT. GraphP features three key techniques. 1) “Source-cut” partitioning, which fundamentally changes the cross-cube communication from one remote put per cross-cube edge to one update per replica. 2) “Two-phase Vertex Program”, a programming model designed for the “source-cut” partitioning with two operations: GenUpdate and ApplyUpdate. 3) Hierarchical communication and overlapping, which further improves performance with unique opportunities offered by the proposed partitioning and programming model. We evaluate GraphP using a cycle accurate simulator with 5 real-world graphs and 4 algorithms. The results show that it provides on average 1.7 speedup and 89\% energy saving compared to TESSERACT.},
	urldate = {2025-03-10},
	booktitle = {2018 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Zhang, Mingxing and Zhuo, Youwei and Wang, Chao and Gao, Mingyu and Wu, Yongwei and Chen, Kang and Kozyrakis, Christos and Qian, Xuehai},
	month = feb,
	year = {2018},
	note = {ISSN: 2378-203X},
	keywords = {Bandwidth, Graph processing, Hybrid Memory Cube, Memory management, Organizations, Partitioning algorithms, Processing In Memory, Programming, Three-dimensional displays},
	pages = {544--557},
}

@inproceedings{li_optimal_2022,
	address = {Taipei, Taiwan},
	series = {{ASPDAC} '22},
	title = {Optimal {Data} {Allocation} for {Graph} {Processing} in {Processing}-in-{Memory} {Systems}},
	isbn = {978-1-66542-135-5},
	url = {https://dl.acm.org/doi/10.1109/ASP-DAC52403.2022.9712587},
	doi = {10.1109/ASP-DAC52403.2022.9712587},
	abstract = {Graph processing involves lots of irregular memory accesses and increases demands on high memory bandwidth, making it difficult to execute efficiently on compute-centric architectures. Dedicated graph processing accelerators based on the processing-in-memory (PIM) technique have recently been proposed. Despite they achieved higher performance and energy efficiency than conventional architectures, the data allocation problem for communication minimization in PIM systems (e.g., hybrid memory cubes (HMCs)) has still not been well solved. In this paper, we demonstrate that the conventional "graph data allocation = graph partitioning" assumption is not true, and the memory access patterns of graph algorithms should also be taken into account when partitioning graph data for communication minimization. For this purpose, we classify graph algorithms into two representative classes from a memory access pattern point of view and propose different graph data partitioning strategies for them. We then propose two algorithms to optimize the partition-to-HMC mapping to minimize the inter-HMC communication. Evaluations have proved the superiority of our data allocation framework and the data movement energy efficiency is improved by 4.2--5× on average than the state-of-the-art GraphP approach.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 27th {Asia} and {South} {Pacific} {Design} {Automation} {Conference}},
	publisher = {IEEE Press},
	author = {Li, Zerun and Chen, Xiaoming and Han, Yinhe},
	month = jan,
	year = {2022},
	pages = {238--243},
}

@inproceedings{malewicz_pregel_2010,
	address = {New York, NY, USA},
	series = {{SIGMOD} '10},
	title = {Pregel: a system for large-scale graph processing},
	isbn = {978-1-4503-0032-2},
	shorttitle = {Pregel},
	url = {https://dl.acm.org/doi/10.1145/1807167.1807184},
	doi = {10.1145/1807167.1807184},
	abstract = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2010 {ACM} {SIGMOD} {International} {Conference} on {Management} of data},
	publisher = {Association for Computing Machinery},
	author = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J.C and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
	month = jun,
	year = {2010},
	pages = {135--146},
}

@inproceedings{will_ruya_2022,
	title = {Ruya: {Memory}-{Aware} {Iterative} {Optimization} of {Cluster} {Configurations} for {Big} {Data} {Processing}},
	shorttitle = {Ruya},
	url = {https://ieeexplore.ieee.org/document/10020295},
	doi = {10.1109/BigData55660.2022.10020295},
	abstract = {Selecting appropriate computational resources for data processing jobs on large clusters is difficult, even for expert users like data engineers. Inadequate choices can result in vastly increased costs, without significantly improving performance. One crucial aspect of selecting an efficient resource configuration is avoiding memory bottlenecks. By knowing the required memory of a job in advance, the search space for an optimal resource configuration can be greatly reduced.Therefore, we present Ruya, a method for memory-aware optimization of data processing cluster configurations based on iteratively exploring a narrowed-down search space. First, we perform job profiling runs with small samples of the dataset on just a single machine to model the job’s memory usage patterns. Second, we prioritize cluster configurations with a suitable amount of total memory and within this reduced search space, we iteratively search for the best cluster configuration with Bayesian optimization. This search process stops once it converges on a configuration that is believed to be optimal for the given job. In our evaluation on a dataset with 1031 Spark and Hadoop jobs, we see a reduction of search iterations to find an optimal configuration by around half, compared to the baseline.},
	urldate = {2025-03-10},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Will, Jonathan and Thamsen, Lauritz and Bader, Jonathan and Scheinert, Dominik and Kao, Odej},
	month = dec,
	year = {2022},
	keywords = {Big Data, Cluster Management, Costs, Current measurement, Data models, Data processing, Distributed Dataflows, Memory management, Profiling, Resource Allocation, Scalable Data Analytics, Sparks},
	pages = {161--169},
}

@inproceedings{al-sayeh_juggler_2022,
	address = {Philadelphia PA USA},
	title = {Juggler: {Autonomous} {Cost} {Optimization} and {Performance} {Prediction} of {Big} {Data} {Applications}},
	isbn = {978-1-4503-9249-5},
	shorttitle = {Juggler},
	url = {https://dl.acm.org/doi/10.1145/3514221.3517892},
	doi = {10.1145/3514221.3517892},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Al-Sayeh, Hani and Memishi, Bunjamin and Jibril, Muhammad Attahir and Paradies, Marcus and Sattler, Kai-Uwe},
	month = jun,
	year = {2022},
	pages = {1840--1854},
}

@misc{al-sayeh_blink_2022,
	title = {Blink: {Lightweight} {Sample} {Runs} for {Cost} {Optimization} of {Big} {Data} {Applications}},
	shorttitle = {Blink},
	url = {http://arxiv.org/abs/2207.02290},
	doi = {10.48550/arXiv.2207.02290},
	abstract = {Distributed in-memory data processing engines accelerate iterative applications by caching substantial datasets in memory rather than recomputing them in each iteration. Selecting a suitable cluster size for caching these datasets plays an essential role in achieving optimal performance. In practice, this is a tedious and hard task for end users, who are typically not aware of cluster specifications, workload semantics and sizes of intermediate data. We present Blink, an autonomous sampling-based framework, which predicts sizes of cached datasets and selects optimal cluster size without relying on historical runs. We evaluate Blink on a variety of iterative, real-world, machine learning applications. With an average sample runs cost of 4.6\% compared to the cost of optimal runs, Blink selects the optimal cluster size in 15 out of 16 cases, saving up to 47.4\% of execution cost compared to average costs.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Al-Sayeh, Hani and Jibril, Muhammad Attahir and Memishi, Bunjamin and Sattler, Kai-Uwe},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02290 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{waleffe_armada_2025,
	title = {Armada: {Memory}-{Efficient} {Distributed} {Training} of {Large}-{Scale} {Graph} {Neural} {Networks}},
	shorttitle = {Armada},
	url = {http://arxiv.org/abs/2502.17846},
	doi = {10.48550/arXiv.2502.17846},
	abstract = {We study distributed training of Graph Neural Networks (GNNs) on billion-scale graphs that are partitioned across machines. Efficient training in this setting relies on min-edge-cut partitioning algorithms, which minimize cross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cut partitioning over large graphs remains a challenge: State-of-the-art (SoTA) offline methods (e.g., METIS) are effective, but they require orders of magnitude more memory and runtime than GNN training itself, while computationally efficient algorithms (e.g., streaming greedy approaches) suffer from increased edge cuts. Thus, in this work we introduce Armada, a new end-to-end system for distributed GNN training whose key contribution is GREM, a novel min-edge-cut partitioning algorithm that can efficiently scale to large graphs. GREM builds on streaming greedy approaches with one key addition: prior vertex assignments are continuously refined during streaming, rather than frozen after an initial greedy selection. Our theoretical analysis and experimental results show that this refinement is critical to minimizing edge cuts and enables GREM to reach partition quality comparable to METIS but with 8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leverages a new disaggregated architecture for distributed GNN training to further improve efficiency; we find that on common cloud machines, even with zero communication, GNN neighborhood sampling and feature loading bottleneck training. Disaggregation allows Armada to independently allocate resources for these operations and ensure that expensive GPUs remain saturated with computation. We evaluate Armada against SoTA systems for distributed GNN training and find that the disaggregated architecture leads to runtime improvements up to 4.5x and cost reductions up to 3.1x.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Waleffe, Roger and Sarda, Devesh and Mohoney, Jason and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Rekatsinas, Theodoros and Venkataraman, Shivaram},
	month = feb,
	year = {2025},
	note = {arXiv:2502.17846 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@inproceedings{zhang_distributed_2021,
	address = {Singapore},
	title = {Distributed {Graph} {Processing}: {Techniques} and {Systems}},
	isbn = {9789811604799},
	shorttitle = {Distributed {Graph} {Processing}},
	doi = {10.1007/978-981-16-0479-9_2},
	abstract = {During the past 10 years, there has been a surging interest in developing distributed graph processing systems. This tutorial provides a comprehensive review of existing distributed graph processing systems. We firstly review the programming models for distributed graph processing and then summarize the common optimization techniques for improving graph execution performance, including graph partitioning methods, communication mechanisms, parallel processing models, hardware-specific optimizations, and incremental graph processing. We also present an emerging hot topic, distributed Graph Neural Networks (GNN) frameworks, and review recent progress on this topic.},
	language = {en},
	booktitle = {Web and {Big} {Data}. {APWeb}-{WAIM} 2020 {International} {Workshops}},
	publisher = {Springer},
	author = {Zhang, Yanfeng and Wang, Qiange and Gong, Shufeng},
	editor = {Chen, Qun and Li, Jianxin},
	year = {2021},
	keywords = {Distributed systems, Graph processing, Parallel models},
	pages = {14--23},
}

@article{zhuo_distributed_2019,
	title = {Distributed {Graph} {Processing} {System} and {Processing}-in-memory {Architecture} with {Precise} {Loop}-carried {Dependency} {Guarantee}},
	volume = {37},
	issn = {0734-2071, 1557-7333},
	url = {https://dl.acm.org/doi/10.1145/3453681},
	doi = {10.1145/3453681},
	abstract = {To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems—
              GraphS
              and
              GraphSR
              —for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.},
	language = {en},
	number = {1-4},
	urldate = {2025-03-10},
	journal = {ACM Transactions on Computer Systems},
	author = {Zhuo, Youwei and Chen, Jingji and Rao, Gengyu and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
	month = nov,
	year = {2019},
	pages = {1--37},
}

@misc{meng_survey_2024,
	title = {A {Survey} of {Distributed} {Graph} {Algorithms} on {Massive} {Graphs}},
	url = {http://arxiv.org/abs/2404.06037},
	doi = {10.48550/arXiv.2404.06037},
	abstract = {Distributed processing of large-scale graph data has many practical applications and has been widely studied. In recent years, a lot of distributed graph processing frameworks and algorithms have been proposed. While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments. Applying graph tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth. In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed graph algorithms. We first conduct a systematic analysis of the inherent challenges in distributed graph processing, followed by presenting an overview of existing general solutions. Subsequently, we survey the challenges highlighted in recent distributed graph processing papers and the strategies adopted to address them. Finally, we discuss the current research trends and identify potential future opportunities.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Meng, Lingkai and Shao, Yu and Yuan, Long and Lai, Longbin and Cheng, Peng and Li, Xue and Yu, Wenyuan and Zhang, Wenjie and Lin, Xuemin and Zhou, Jingren},
	month = oct,
	year = {2024},
	note = {arXiv:2404.06037 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{baluja_video_2008,
	address = {New York, NY, USA},
	series = {{WWW} '08},
	title = {Video suggestion and discovery for youtube: taking random walks through the view graph},
	isbn = {978-1-60558-085-2},
	shorttitle = {Video suggestion and discovery for youtube},
	url = {https://dl.acm.org/doi/10.1145/1367497.1367618},
	doi = {10.1145/1367497.1367618},
	abstract = {The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them. Unfortunately, given the difficulty of searching videos, the size of the video repository also makes the discovery of new content a daunting task. In this paper, we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users. The resulting algorithm, termed Adsorption, provides a simple method to efficiently propagate preference information through a variety of graphs. We extensively test the results of the recommendations on a three month snapshot of live data from YouTube.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 17th international conference on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Baluja, Shumeet and Seth, Rohan and Sivakumar, D. and Jing, Yushi and Yagnik, Jay and Kumar, Shankar and Ravichandran, Deepak and Aly, Mohamed},
	month = apr,
	year = {2008},
	pages = {895--904},
}

@article{zhang_cgraph_2019,
	title = {{CGraph}: {A} {Distributed} {Storage} and {Processing} {System} for {Concurrent} {Iterative} {Graph} {Analysis} {Jobs}},
	volume = {15},
	issn = {1553-3077},
	shorttitle = {{CGraph}},
	url = {https://doi.org/10.1145/3319406},
	doi = {10.1145/3319406},
	abstract = {Distributed graph processing platforms usually need to handle massive Concurrent iterative Graph Processing (CGP) jobs for different purposes. However, existing distributed systems face high ratio of data access cost to computation for the CGP jobs, which incurs low throughput. We observed that there are strong spatial and temporal correlations among the data accesses issued by different CGP jobs, because these concurrently running jobs usually need to repeatedly traverse the shared graph structure for the iterative processing of each vertex. Based on this observation, this article proposes a distributed storage and processing system CGraph for the CGP jobs to efficiently handle the underlying static/evolving graph for high throughput. It uses a data-centric load-trigger-pushing model, together with several optimizations, to enable the CGP jobs to efficiently share the graph structure data in the cache/memory and their accesses by fully exploiting such correlations, where the graph structure data is decoupled from the vertex state associated with each job. It can deliver much higher throughput for the CGP jobs by effectively reducing their average ratio of data access cost to computation. Experimental results show that CGraph improves the throughput of the CGP jobs by up to 3.47× in comparison with existing solutions on distributed platforms.},
	number = {2},
	urldate = {2025-03-10},
	journal = {ACM Trans. Storage},
	author = {Zhang, Yu and Zhao, Jin and Liao, Xiaofei and Jin, Hai and Gu, Lin and Liu, Haikun and He, Bingsheng and He, Ligang},
	month = apr,
	year = {2019},
	pages = {10:1--10:26},
}

@article{wang_fargraph_2023,
	title = {Fargraph+: {Excavating} the parallelism of graph processing workload on {RDMA}-based far memory system},
	volume = {177},
	issn = {0743-7315},
	shorttitle = {Fargraph+},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731523000345},
	doi = {10.1016/j.jpdc.2023.02.015},
	abstract = {Disaggregated architecture brings new opportunities to memory-consuming applications like graph processing. It allows one to outspread memory access pressure from local to far memory, providing an attractive alternative to disk-based processing. Although existing works on general-purpose far memory platforms show great potentials for application expansion, it is unclear how graph processing applications could benefit from disaggregated architecture, and how different optimization methods influence the overall performance. In this paper, we take the first step to analyze the impact of graph processing workload on disaggregated architecture by extending the GridGraph framework on top of the RDMA-based far memory system. We propose Fargraph+, a system with parallel graph data offloading and far memory coordination strategy for enhancing efficiency of graph processing workload on RDMA-based far memory architecture. Specifically, Fargraph+ reduces the overall data movement through a well-crafted, graph-aware data segment offloading mechanism. In addition, we use optimal data segment splitting and asynchronous data buffering to achieve graph iteration-friendly far memory access. We further configure efficient parallelism-oriented control to accelerate performance of multi-threading processing on graph iterations while improving memory efficiency of far memory access by utilizing RDMA queue features. We show that Fargraph+ achieves near-oracle performance for typical in-local-memory graph processing systems. Fargraph+ shows up to 11.2× speedup compared to Fastswap, the state-of-the-art, general-purpose far memory platform.},
	urldate = {2025-03-10},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Jing and Li, Chao and Liu, Yibo and Wang, Taolei and Mei, Junyi and Zhang, Lu and Wang, Pengyu and Guo, Minyi},
	month = jul,
	year = {2023},
	keywords = {Far memory, Graph processing, RDMA},
	pages = {144--159},
}

@inproceedings{will_selecting_2023,
	address = {New York, NY, USA},
	series = {{SSDBM} '23},
	title = {Selecting {Efficient} {Cluster} {Resources} for {Data} {Analytics}: {When} and {How} to {Allocate} for {In}-{Memory} {Processing}?},
	isbn = {9798400707469},
	shorttitle = {Selecting {Efficient} {Cluster} {Resources} for {Data} {Analytics}},
	url = {https://dl.acm.org/doi/10.1145/3603719.3603733},
	doi = {10.1145/3603719.3603733},
	abstract = {Distributed dataflow systems such as Apache Spark or Apache Flink enable parallel, in-memory data processing on large clusters of commodity hardware. Consequently, the appropriate amount of memory to allocate to the cluster is a crucial consideration. In this paper, we analyze the challenge of efficient resource allocation for distributed data processing, focusing on memory. We\&nbsp;emphasize that in-memory processing with in-memory data processing frameworks can undermine resource efficiency. Based on the findings of our trace data analysis, we compile requirements towards an automated solution for efficient cluster resource allocation.},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Will, Jonathan and Thamsen, Lauritz and Scheinert, Dominik and Kao, Odej},
	month = aug,
	year = {2023},
	pages = {1--4},
}

@misc{university_of_alabama_libraries_how_2021,
	title = {How to {Zotero}: {Adding} {Items} to {Your} {Library}},
	shorttitle = {How to {Zotero}},
	url = {https://www.youtube.com/watch?v=-cI23OS7724},
	abstract = {Creative Commons-Lizenz mit Quellenangabe (Wiederverwendung erlaubt)},
	urldate = {2025-03-10},
	author = {{University of Alabama Libraries}},
	month = aug,
	year = {2021},
}

@article{noauthor_notitle_nodate,
}
